{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn torch transformers tqdm nltk onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ac349",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed16d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set matplotlib styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data path (adjust this to your specific environment)\n",
    "data_path = 'Data/'  # Assuming files are in current directory\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(data_path + 'train.csv')\n",
    "test_df = pd.read_csv(data_path + 'test.csv')\n",
    "misconceptions_df = pd.read_csv(data_path + 'misconception_mapping.csv')\n",
    "sample_submission_df = pd.read_csv(data_path + 'sample_submission.csv')\n",
    "\n",
    "# Print basic information\n",
    "print(f\"Train dataset shape: {train_df.shape}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "print(f\"Misconceptions dataset shape: {misconceptions_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission_df.shape}\")\n",
    "\n",
    "# Check the column names in each file to understand structure\n",
    "print(\"\\nColumns in train.csv:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColumns in test.csv:\")\n",
    "print(test_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColumns in misconception_mapping.csv:\")\n",
    "print(misconceptions_df.columns.tolist())\n",
    "\n",
    "print(\"\\nColumns in sample_submission.csv:\")\n",
    "print(sample_submission_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d90b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the train dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the train dataset\n",
    "print(\"Columns in the train dataset:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in train dataset:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics of train dataset:\")\n",
    "print(train_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the test dataset\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the test dataset\n",
    "print(\"Columns in the test dataset:\")\n",
    "print(test_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in test dataset:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset to understand their structure\n",
    "print(\"\\nFirst few rows of train_df:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nFirst few rows of test_df:\")\n",
    "display(test_df.head())\n",
    "\n",
    "print(\"\\nFirst few rows of misconceptions_df:\")\n",
    "display(misconceptions_df.head())\n",
    "\n",
    "print(\"\\nFirst few rows of sample_submission_df:\")\n",
    "display(sample_submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb794ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the misconceptions dataset\n",
    "print(\"Columns in the misconceptions dataset:\")\n",
    "print(misconceptions_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in misconceptions dataset:\")\n",
    "print(misconceptions_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0769d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some random examples of misconceptions\n",
    "misconceptions_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure we have the necessary columns\n",
    "if 'QuestionId' in train_df.columns and 'MisconceptionId' in train_df.columns:\n",
    "    # How many misconceptions are associated with each question in the training set?\n",
    "    misconceptions_per_question = train_df.groupby('QuestionId')['MisconceptionId'].count()\n",
    "    \n",
    "    # Visualize the distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(misconceptions_per_question, bins=20, kde=True)\n",
    "    plt.title('Number of Misconceptions per Question')\n",
    "    plt.xlabel('Count of Misconceptions')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Average number of misconceptions per question: {misconceptions_per_question.mean():.2f}\")\n",
    "    print(f\"Median number of misconceptions per question: {misconceptions_per_question.median()}\")\n",
    "    print(f\"Max number of misconceptions per question: {misconceptions_per_question.max()}\")\n",
    "    \n",
    "    # Additional analysis: Most common misconceptions\n",
    "    misconception_counts = train_df['MisconceptionId'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    misconception_counts.head(15).plot(kind='bar')\n",
    "    plt.title('Top 15 Most Common Misconceptions')\n",
    "    plt.xlabel('Misconception ID')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Join with misconception descriptions if available\n",
    "    if 'MisconceptionId' in misconceptions_df.columns:\n",
    "        # Check for the column that contains misconception descriptions\n",
    "        desc_column = None\n",
    "        for col in misconceptions_df.columns:\n",
    "            if 'text' in col.lower() or 'description' in col.lower():\n",
    "                desc_column = col\n",
    "                break\n",
    "        \n",
    "        if desc_column:\n",
    "            # Show top misconceptions with their descriptions\n",
    "            top_misconceptions = misconception_counts.head(10).index\n",
    "            for mid in top_misconceptions:\n",
    "                desc = misconceptions_df[misconceptions_df['MisconceptionId'] == mid][desc_column].values\n",
    "                if len(desc) > 0:\n",
    "                    print(f\"Misconception ID {mid} (Count: {misconception_counts[mid]}):\")\n",
    "                    print(f\"  Description: {desc[0]}\")\n",
    "                    print()\n",
    "else:\n",
    "    print(\"Required columns 'QuestionId' or 'MisconceptionId' not found in the training data.\")\n",
    "    print(\"Available columns:\", train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66962149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what columns we actually have in each file\n",
    "print(\"Columns in train_df:\", train_df.columns.tolist())\n",
    "print(\"Columns in test_df:\", test_df.columns.tolist())\n",
    "print(\"Columns in misconceptions_df:\", misconceptions_df.columns.tolist())\n",
    "print(\"Columns in sample_submission_df:\", sample_submission_df.columns.tolist())\n",
    "\n",
    "# Let's also look at a few rows from each file\n",
    "print(\"\\nSample rows from train_df:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nSample rows from misconceptions_df:\")\n",
    "display(misconceptions_df.head())\n",
    "\n",
    "print(\"\\nSample rows from test_df:\")\n",
    "display(test_df.head())\n",
    "\n",
    "print(\"\\nSample rows from sample_submission_df:\")\n",
    "display(sample_submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ea111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify the correct column names in your data\n",
    "print(\"Columns in train_df:\", train_df.columns.tolist())\n",
    "print(\"Columns in misconceptions_df:\", misconceptions_df.columns.tolist())\n",
    "\n",
    "# Try to identify misconception ID column (common variations)\n",
    "misconception_id_col = None\n",
    "potential_names = ['MisconceptionId', 'misconception_id', 'misconceptionid', 'Misconception', 'MiscID', 'misc_id']\n",
    "for name in potential_names:\n",
    "    if name in train_df.columns:\n",
    "        misconception_id_col = name\n",
    "        print(f\"Found misconception ID column: '{name}'\")\n",
    "        break\n",
    "\n",
    "if misconception_id_col is None:\n",
    "    print(\"Could not find misconception ID column. Available columns:\", train_df.columns.tolist())\n",
    "    # Let's suggest the most likely column based on content\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].dtype in ['int64', 'int32'] and train_df[col].nunique() > 5:\n",
    "            print(f\"Potential misconception ID column: '{col}' (has {train_df[col].nunique()} unique values)\")\n",
    "else:\n",
    "    # Compute misconception counts using the identified column\n",
    "    misconception_counts = train_df[misconception_id_col].value_counts()\n",
    "    \n",
    "    # Now plot the top 20 misconceptions\n",
    "    top_misconceptions = misconception_counts.head(20)\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    top_misconceptions.plot(kind='bar')\n",
    "    plt.title('Top 20 Most Common Misconceptions')\n",
    "    plt.xlabel('Misconception ID')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total unique misconceptions: {len(misconception_counts)}\")\n",
    "    print(f\"Average frequency: {misconception_counts.mean():.2f}\")\n",
    "    print(f\"Median frequency: {misconception_counts.median()}\")\n",
    "    print(f\"Most common misconception appears {misconception_counts.max()} times\")\n",
    "    print(f\"Number of misconceptions that appear only once: {sum(misconception_counts == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check if subject and construct information is available in your files\n",
    "subject_in_train = 'SubjectId' in train_df.columns\n",
    "construct_in_train = 'ConstructId' in train_df.columns\n",
    "subject_in_test = 'SubjectId' in test_df.columns\n",
    "construct_in_test = 'ConstructId' in test_df.columns\n",
    "\n",
    "print(f\"SubjectId in train_df: {subject_in_train}\")\n",
    "print(f\"ConstructId in train_df: {construct_in_train}\")\n",
    "print(f\"SubjectId in test_df: {subject_in_test}\")\n",
    "print(f\"ConstructId in test_df: {construct_in_test}\")\n",
    "\n",
    "# If the columns exist in train data, plot distributions\n",
    "if subject_in_train:\n",
    "    # Distribution of questions across subjects in train data\n",
    "    subject_counts = train_df['SubjectId'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    subject_counts.plot(kind='bar')\n",
    "    plt.title('Distribution of Questions Across Subjects (Train Data)')\n",
    "    plt.xlabel('Subject ID')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print subject distribution statistics\n",
    "    print(f\"Number of different subjects: {len(subject_counts)}\")\n",
    "    print(f\"Most common subject has {subject_counts.max()} questions\")\n",
    "    print(f\"Least common subject has {subject_counts.min()} questions\")\n",
    "else:\n",
    "    print(\"SubjectId not found in train data\")\n",
    "\n",
    "if construct_in_train:\n",
    "    # Distribution of questions across constructs in train data\n",
    "    construct_counts = train_df['ConstructId'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    top_constructs = construct_counts.head(20)\n",
    "    top_constructs.plot(kind='bar')\n",
    "    plt.title('Top 20 Most Common Constructs (Train Data)')\n",
    "    plt.xlabel('Construct ID')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print construct distribution statistics\n",
    "    print(f\"Number of different constructs: {len(construct_counts)}\")\n",
    "    print(f\"Most common construct has {construct_counts.max()} questions\")\n",
    "    print(f\"Least common construct has {construct_counts.min()} questions\")\n",
    "else:\n",
    "    print(\"ConstructId not found in train data\")\n",
    "\n",
    "# If columns only exist in test data, analyze those instead\n",
    "if not subject_in_train and subject_in_test:\n",
    "    # Distribution of questions across subjects in test data\n",
    "    subject_counts = test_df['SubjectId'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    subject_counts.plot(kind='bar')\n",
    "    plt.title('Distribution of Questions Across Subjects (Test Data)')\n",
    "    plt.xlabel('Subject ID')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "if not construct_in_train and construct_in_test:\n",
    "    # Distribution of questions across constructs in test data\n",
    "    construct_counts = test_df['ConstructId'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(14, 7))\n",
    "    top_constructs = construct_counts.head(20)\n",
    "    top_constructs.plot(kind='bar')\n",
    "    plt.title('Top 20 Most Common Constructs (Test Data)')\n",
    "    plt.xlabel('Construct ID')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# If neither train nor test data has these columns, suggest alternatives\n",
    "if not (subject_in_train or subject_in_test):\n",
    "    print(\"Subject information not found in any dataset. Consider exploring other columns.\")\n",
    "    # List columns that might be alternatives\n",
    "    print(\"Available columns in train_df:\", train_df.columns.tolist())\n",
    "    print(\"Available columns in test_df:\", test_df.columns.tolist())\n",
    "    \n",
    "    # Suggest alternative analyses based on available columns\n",
    "    categorical_cols = [col for col in train_df.columns if train_df[col].dtype == 'object' or train_df[col].nunique() < 50]\n",
    "    if categorical_cols:\n",
    "        print(\"\\nPossible categorical columns to analyze instead:\", categorical_cols)\n",
    "        \n",
    "        # Select first categorical column as an example\n",
    "        example_col = categorical_cols[0]\n",
    "        example_counts = train_df[example_col].value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        example_counts.head(20).plot(kind='bar')\n",
    "        plt.title(f'Distribution of {example_col} in Train Data')\n",
    "        plt.xlabel(example_col)\n",
    "        plt.ylabel('Count')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for question text columns in train_df and test_df\n",
    "question_text_cols = []\n",
    "for col in train_df.columns:\n",
    "    if 'question' in col.lower() and 'text' in col.lower():\n",
    "        question_text_cols.append(col)\n",
    "if not question_text_cols:\n",
    "    for col in test_df.columns:\n",
    "        if 'question' in col.lower() and 'text' in col.lower():\n",
    "            question_text_cols.append(col)\n",
    "\n",
    "# Check for misconception text column in misconceptions_df\n",
    "misconception_text_col = None\n",
    "for col in misconceptions_df.columns:\n",
    "    if 'text' in col.lower() or 'description' in col.lower():\n",
    "        misconception_text_col = col\n",
    "        break\n",
    "\n",
    "# Length distribution of question texts (if available)\n",
    "if question_text_cols:\n",
    "    # Use the first matching column\n",
    "    question_text_col = question_text_cols[0]\n",
    "    print(f\"Using column '{question_text_col}' for question text analysis\")\n",
    "    \n",
    "    # Determine which dataframe has this column\n",
    "    if question_text_col in train_df.columns:\n",
    "        df_with_questions = train_df\n",
    "    else:\n",
    "        df_with_questions = test_df\n",
    "    \n",
    "    # Calculate text lengths\n",
    "    df_with_questions[f'{question_text_col}Length'] = df_with_questions[question_text_col].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df_with_questions[f'{question_text_col}Length'].dropna(), bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {question_text_col} Lengths')\n",
    "    plt.xlabel('Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    length_data = df_with_questions[f'{question_text_col}Length'].dropna()\n",
    "    print(f\"Average {question_text_col} length: {length_data.mean():.2f} characters\")\n",
    "    print(f\"Median {question_text_col} length: {length_data.median()} characters\")\n",
    "    print(f\"Max {question_text_col} length: {length_data.max()} characters\")\n",
    "else:\n",
    "    print(\"No question text column found in available dataframes.\")\n",
    "    print(\"Available columns in train_df:\", train_df.columns.tolist())\n",
    "    print(\"Available columns in test_df:\", test_df.columns.tolist())\n",
    "\n",
    "# Length distribution of misconception texts\n",
    "if misconception_text_col:\n",
    "    print(f\"\\nUsing column '{misconception_text_col}' for misconception text analysis\")\n",
    "    \n",
    "    # Calculate text lengths\n",
    "    misconceptions_df['MisconceptionTextLength'] = misconceptions_df[misconception_text_col].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(misconceptions_df['MisconceptionTextLength'].dropna(), bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {misconception_text_col} Lengths')\n",
    "    plt.xlabel('Length (characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    length_data = misconceptions_df['MisconceptionTextLength'].dropna()\n",
    "    print(f\"Average misconception text length: {length_data.mean():.2f} characters\")\n",
    "    print(f\"Median misconception text length: {length_data.median()} characters\")\n",
    "    print(f\"Max misconception text length: {length_data.max()} characters\")\n",
    "else:\n",
    "    print(\"\\nNo misconception text column found in misconceptions_df.\")\n",
    "    print(\"Available columns in misconceptions_df:\", misconceptions_df.columns.tolist())\n",
    "    \n",
    "    # Provide a fallback analysis on misconception IDs if text isn't available\n",
    "    if 'MisconceptionId' in misconceptions_df.columns or any('misconception' in col.lower() for col in misconceptions_df.columns):\n",
    "        mid_col = 'MisconceptionId' if 'MisconceptionId' in misconceptions_df.columns else [col for col in misconceptions_df.columns if 'misconception' in col.lower()][0]\n",
    "        \n",
    "        print(f\"\\nFallback: Analyzing misconception IDs using column '{mid_col}'\")\n",
    "        print(f\"Total unique misconceptions: {misconceptions_df[mid_col].nunique()}\")\n",
    "        \n",
    "        # Plot distribution of misconception frequencies if we have train data with this column\n",
    "        if mid_col in train_df.columns:\n",
    "            misconception_counts = train_df[mid_col].value_counts()\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(misconception_counts, bins=20, kde=True)\n",
    "            plt.title('Distribution of Misconception Frequencies')\n",
    "            plt.xlabel('Frequency')\n",
    "            plt.ylabel('Count')\n",
    "            plt.grid(True)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what columns we have in our dataframes\n",
    "print(\"Columns in train_df:\", train_df.columns.tolist())\n",
    "print(\"Columns in misconceptions_df:\", misconceptions_df.columns.tolist())\n",
    "\n",
    "# Identify relevant column names\n",
    "# Check for question ID column\n",
    "question_id_col = None\n",
    "for col in train_df.columns:\n",
    "    if 'question' in col.lower() and 'id' in col.lower():\n",
    "        question_id_col = col\n",
    "        break\n",
    "\n",
    "# Check for misconception ID column\n",
    "misconception_id_col = None\n",
    "for col in train_df.columns:\n",
    "    if 'misconception' in col.lower() and 'id' in col.lower():\n",
    "        misconception_id_col = col\n",
    "        break\n",
    "\n",
    "# Check if we need to merge with another file for question text\n",
    "question_text_in_train = any('text' in col.lower() and 'question' in col.lower() for col in train_df.columns)\n",
    "if not question_text_in_train:\n",
    "    print(\"Question text not found in train_df. Cannot merge with questions_df as it's not available.\")\n",
    "    print(\"Available data can only be analyzed based on IDs.\")\n",
    "\n",
    "# Check for question text and answer text columns\n",
    "question_text_col = None\n",
    "answer_text_col = None\n",
    "for col in train_df.columns:\n",
    "    if 'question' in col.lower() and 'text' in col.lower():\n",
    "        question_text_col = col\n",
    "    if 'answer' in col.lower() and 'text' in col.lower():\n",
    "        answer_text_col = col\n",
    "\n",
    "# Check for misconception text column\n",
    "misconception_text_col = None\n",
    "for col in misconceptions_df.columns:\n",
    "    if 'text' in col.lower() or 'description' in col.lower():\n",
    "        misconception_text_col = col\n",
    "        break\n",
    "\n",
    "# If we have the necessary columns, sample some records\n",
    "if question_id_col and misconception_id_col:\n",
    "    # Sample directly from train_df without merging\n",
    "    sample_records = train_df.sample(5)\n",
    "    \n",
    "    for _, row in sample_records.iterrows():\n",
    "        print(f\"Question ID: {row[question_id_col]}\")\n",
    "        \n",
    "        # Print question text if available\n",
    "        if question_text_col and question_text_col in row:\n",
    "            print(f\"Question: {row[question_text_col]}\")\n",
    "        \n",
    "        # Print answer text if available\n",
    "        if answer_text_col and answer_text_col in row:\n",
    "            print(f\"Answer: {row[answer_text_col]}\")\n",
    "        \n",
    "        # Get the misconception information if available\n",
    "        if misconception_id_col in row and misconception_text_col:\n",
    "            try:\n",
    "                misconception = misconceptions_df[misconceptions_df[misconception_id_col] == row[misconception_id_col]]\n",
    "                if not misconception.empty:\n",
    "                    print(f\"Misconception ID: {row[misconception_id_col]}\")\n",
    "                    print(f\"Misconception: {misconception.iloc[0][misconception_text_col]}\")\n",
    "                else:\n",
    "                    print(f\"Misconception ID: {row[misconception_id_col]}\")\n",
    "                    print(f\"Misconception: [Not found in misconceptions_df]\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving misconception: {str(e)}\")\n",
    "        \n",
    "        print(\"-\" * 100)\n",
    "else:\n",
    "    print(\"Required ID columns not found. Cannot analyze relationships between questions and misconceptions.\")\n",
    "    print(f\"Looking for question ID column like 'QuestionId' and misconception ID column like 'MisconceptionId'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check the columns and size of your test dataframe\n",
    "print(\"Columns in test_df:\", test_df.columns.tolist())\n",
    "print(f\"Number of rows in test_df: {len(test_df)}\")\n",
    "\n",
    "# Sample some records from the test set, making sure not to request more than available\n",
    "sample_size = min(5, len(test_df))\n",
    "if sample_size > 0:\n",
    "    test_samples = test_df.sample(sample_size)\n",
    "    print(f\"\\nSample records from test data ({sample_size} samples):\")\n",
    "    display(test_samples)\n",
    "else:\n",
    "    print(\"\\nTest dataset is empty, cannot sample records\")\n",
    "    \n",
    "# Let's also check the size of other dataframes for reference\n",
    "print(f\"\\nNumber of rows in train_df: {len(train_df)}\")\n",
    "print(f\"Number of rows in misconceptions_df: {len(misconceptions_df)}\")\n",
    "\n",
    "# Display the first few rows of each dataframe instead of sampling\n",
    "print(\"\\nFirst few rows of train_df:\")\n",
    "display(train_df.head(2))\n",
    "\n",
    "print(\"\\nFirst few rows of test_df:\")\n",
    "display(test_df.head(2))\n",
    "\n",
    "print(\"\\nFirst few rows of misconceptions_df:\")\n",
    "display(misconceptions_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba83d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any QuestionIds in the test set that are not in the training set\n",
    "test_only_questions = set(test_df['QuestionId']) - set(train_df['QuestionId'])\n",
    "print(f\"Number of questions only in the test set: {len(test_only_questions)}\")\n",
    "\n",
    "# Check if there are any QuestionIds in the training set that are not in the test set\n",
    "train_only_questions = set(train_df['QuestionId']) - set(test_df['QuestionId'])\n",
    "print(f\"Number of questions only in the training set: {len(train_only_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a611c5",
   "metadata": {},
   "source": [
    "##SECTION-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b57c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing special characters, extra spaces, etc.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags if any\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and numbers (keeping math symbols)\n",
    "        # We keep +, -, *, /, =, <, >, (, ), [, ], {, }, ^, √ as they're important for math\n",
    "        text = re.sub(r'[^\\w\\s+\\-*/=<>()[\\]{}\\^√]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Preprocess text with optional stopword removal and lemmatization.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean the text first\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize if requested\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "\n",
    "# Create cross-validation splits adapted to your dataset structure\n",
    "def create_validation_splits(df, id_column, n_splits=5, method='group'):\n",
    "    \"\"\"\n",
    "    Create validation splits for cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        id_column: Column name to use for grouping (e.g., question_id_col or misconception_id_col)\n",
    "        n_splits: Number of splits to create\n",
    "        method: Method to use for splitting ('group' or 'stratified')\n",
    "    \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    if id_column not in df.columns:\n",
    "        print(f\"Column {id_column} not found in dataframe\")\n",
    "        return []\n",
    "        \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Check for and handle NaN values in the grouping column\n",
    "    if df_copy[id_column].isna().any():\n",
    "        print(f\"Found {df_copy[id_column].isna().sum()} NaN values in {id_column}. Filling with unique identifier.\")\n",
    "        df_copy[id_column] = df_copy[id_column].fillna(-999)  # Use a value that doesn't exist in your data\n",
    "    \n",
    "    if method == 'group':\n",
    "        # Use GroupKFold to ensure related items stay together\n",
    "        cv = GroupKFold(n_splits=n_splits)\n",
    "        splits = list(cv.split(df_copy, groups=df_copy[id_column]))\n",
    "    elif method == 'stratified':\n",
    "        # Use StratifiedKFold to maintain the distribution\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = list(cv.split(df_copy, df_copy[id_column]))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown split method: {method}\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def create_custom_unseen_split(df, id_column, unseen_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a custom train/validation split where some items are completely held out.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        id_column: Column to use for creating the unseen split\n",
    "        unseen_ratio: Proportion to hold out\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_idx, valid_idx: Indices for training and validation sets\n",
    "    \"\"\"\n",
    "    if id_column not in df.columns:\n",
    "        print(f\"Column {id_column} not found in dataframe\")\n",
    "        return [], []\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Handle NaN values in the ID column\n",
    "    if df_copy[id_column].isna().any():\n",
    "        print(f\"Found {df_copy[id_column].isna().sum()} NaN values in {id_column}. Filling with unique identifier.\")\n",
    "        df_copy[id_column] = df_copy[id_column].fillna(-999)  # Use a value that doesn't exist in your data\n",
    "    \n",
    "    # Get unique values (excluding any filled NaN values if you want to keep them separate)\n",
    "    unique_values = df_copy[df_copy[id_column] != -999][id_column].unique()\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Randomly select values to hold out\n",
    "    n_unseen = int(len(unique_values) * unseen_ratio)\n",
    "    unseen_values = np.random.choice(unique_values, size=n_unseen, replace=False)\n",
    "    \n",
    "    # Create train and validation indices\n",
    "    train_idx = df[~df[id_column].isin(unseen_values)].index\n",
    "    valid_idx = df[df[id_column].isin(unseen_values)].index\n",
    "    \n",
    "    return train_idx, valid_idx\n",
    "\n",
    "# Main execution code\n",
    "if 'question_id_col' in locals() and 'misconception_id_col' in locals() and question_id_col and misconception_id_col:\n",
    "    # Choose which column to use for splits (usually misconception ID for this task)\n",
    "    split_column = misconception_id_col\n",
    "    \n",
    "    # Print diagnostic information\n",
    "    print(f\"Using {split_column} for splits\")\n",
    "    print(f\"Data type: {train_df[split_column].dtype}\")\n",
    "    print(f\"NaN values: {train_df[split_column].isna().sum()}\")\n",
    "    print(f\"Unique values (sample): {sorted(list(train_df[split_column].dropna().unique())[:5])}\")\n",
    "    \n",
    "    # Create standard cross-validation splits\n",
    "    group_splits = create_validation_splits(train_df, split_column, method='group')\n",
    "    stratified_splits = create_validation_splits(train_df, split_column, method='stratified')\n",
    "    \n",
    "    # Create a custom split with unseen items\n",
    "    unseen_train_idx, unseen_valid_idx = create_custom_unseen_split(train_df, split_column, unseen_ratio=0.2)\n",
    "    \n",
    "    # Analyze the splits\n",
    "    if group_splits:\n",
    "        print(\"\\nStandard GroupKFold splits:\")\n",
    "        for fold, (train_idx, valid_idx) in enumerate(group_splits):\n",
    "            train_values = set(train_df.iloc[train_idx][split_column].dropna())\n",
    "            valid_values = set(train_df.iloc[valid_idx][split_column].dropna())\n",
    "            unseen_values = valid_values - train_values\n",
    "            \n",
    "            print(f\"Fold {fold+1}:\")\n",
    "            print(f\"  Train set size: {len(train_idx)}\")\n",
    "            print(f\"  Validation set size: {len(valid_idx)}\")\n",
    "            print(f\"  Number of unseen values in validation: {len(unseen_values)}\")\n",
    "            print(f\"  Proportion of unseen values: {len(unseen_values) / len(valid_values) if len(valid_values) > 0 else 0:.2f}\")\n",
    "    \n",
    "    print(\"\\nCustom split with unseen values:\")\n",
    "    if len(unseen_train_idx) > 0 and len(unseen_valid_idx) > 0:\n",
    "        train_values = set(train_df.iloc[unseen_train_idx][split_column].dropna())\n",
    "        valid_values = set(train_df.iloc[unseen_valid_idx][split_column].dropna())\n",
    "        unseen_values = valid_values - train_values\n",
    "        \n",
    "        print(f\"Train set size: {len(unseen_train_idx)}\")\n",
    "        print(f\"Validation set size: {len(unseen_valid_idx)}\")\n",
    "        print(f\"Number of unseen values in validation: {len(unseen_values)}\")\n",
    "        print(f\"Proportion of unseen values: {len(unseen_values) / len(valid_values) if len(valid_values) > 0 else 0:.2f}\")\n",
    "else:\n",
    "    print(\"Cannot create validation splits: Required ID columns not found\")\n",
    "    print(\"Available columns:\", train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612be4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(row, include_subject=True, include_construct=True):\n",
    "    \"\"\"\n",
    "    Create a query representation from a row containing question and answer information.\n",
    "    \"\"\"\n",
    "    # Start with the question text\n",
    "    query_parts = []\n",
    "    \n",
    "    # Add subject and construct if available and requested\n",
    "    if include_subject and 'SubjectId' in row and pd.notna(row['SubjectId']):\n",
    "        subject_name = row.get('SubjectName', str(row['SubjectId']))\n",
    "        query_parts.append(f\"Subject: {subject_name}\")\n",
    "    \n",
    "    if include_construct and 'ConstructId' in row and pd.notna(row['ConstructId']):\n",
    "        construct_name = row.get('ConstructName', str(row['ConstructId']))\n",
    "        query_parts.append(f\"Construct: {construct_name}\")\n",
    "    \n",
    "    # Add question text\n",
    "    if 'QuestionText' in row and pd.notna(row['QuestionText']):\n",
    "        query_parts.append(f\"Question: {row['QuestionText']}\")\n",
    "    \n",
    "    # Add the specific answer text that has the misconception\n",
    "    if 'AnswerText' in row and pd.notna(row['AnswerText']):\n",
    "        query_parts.append(f\"Incorrect Answer: {row['AnswerText']}\")\n",
    "    \n",
    "    # Join all parts\n",
    "    query = \" \".join(query_parts)\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of merging with a non-existent questions_df, flatten the train data\n",
    "flattened_train_df = []\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    question_id = row['QuestionId']\n",
    "    \n",
    "    # For each answer option that has a misconception ID\n",
    "    for letter in ['A', 'B', 'C', 'D']:\n",
    "        misc_col = f'Misconception{letter}Id'\n",
    "        answer_col = f'Answer{letter}Text'\n",
    "        \n",
    "        # Check if this answer has a misconception\n",
    "        if misc_col in row and pd.notna(row[misc_col]):\n",
    "            # Create a new row for this question-misconception pair\n",
    "            new_row = row.copy()\n",
    "            new_row['MisconceptionId'] = row[misc_col]\n",
    "            new_row['AnswerText'] = row[answer_col]\n",
    "            flattened_train_df.append(new_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "flattened_train_df = pd.DataFrame(flattened_train_df)\n",
    "\n",
    "# Create query representations\n",
    "flattened_train_df['Query'] = flattened_train_df.apply(create_query, axis=1)\n",
    "test_df['Query'] = test_df.apply(create_query, axis=1)\n",
    "\n",
    "# Check a few examples\n",
    "print(\"Sample query representations from training data:\")\n",
    "for i, row in flattened_train_df.sample(min(3, len(flattened_train_df))).iterrows():\n",
    "    print(f\"Query: {row['Query']}\")\n",
    "    # Get the misconception text\n",
    "    misconception_rows = misconceptions_df[misconceptions_df['MisconceptionId'] == row['MisconceptionId']]\n",
    "    if not misconception_rows.empty:\n",
    "        # Use MisconceptionName from your data structure\n",
    "        print(f\"Associated Misconception: {misconception_rows.iloc[0]['MisconceptionName']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5096dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple preprocessing function that doesn't use NLTK\n",
    "import re\n",
    "\n",
    "def preprocess_text_simple(text):\n",
    "    \"\"\"\n",
    "    A simpler preprocessing function that doesn't rely on NLTK.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or text == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters (keeping math symbols)\n",
    "    text = re.sub(r'[^\\w\\s+\\-*/=<>()[\\]{}\\^√]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# First, flatten the train data to create one row per question-misconception pair\n",
    "flattened_train_df = []\n",
    "for _, row in train_df.iterrows():\n",
    "    question_id = row['QuestionId']\n",
    "    \n",
    "    # For each answer option that has a misconception ID\n",
    "    for letter in ['A', 'B', 'C', 'D']:\n",
    "        misc_col = f'Misconception{letter}Id'\n",
    "        answer_col = f'Answer{letter}Text'\n",
    "        \n",
    "        # Check if this column exists and has a valid misconception\n",
    "        if misc_col in row.index and pd.notna(row[misc_col]):\n",
    "            # Create a new row for this question-misconception pair\n",
    "            new_row = row.copy()\n",
    "            new_row['MisconceptionId'] = row[misc_col]\n",
    "            new_row['AnswerText'] = row[answer_col]\n",
    "            flattened_train_df.append(new_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "flattened_train_df = pd.DataFrame(flattened_train_df)\n",
    "\n",
    "# Create query representations using the flattened data\n",
    "flattened_train_df['Query'] = flattened_train_df.apply(create_query, axis=1)\n",
    "test_df['Query'] = test_df.apply(create_query, axis=1)\n",
    "\n",
    "# Use the simple preprocessing function instead of the NLTK-dependent one\n",
    "flattened_train_df['ProcessedQuery'] = flattened_train_df['Query'].apply(preprocess_text_simple)\n",
    "test_df['ProcessedQuery'] = test_df['Query'].apply(preprocess_text_simple)\n",
    "\n",
    "# Check if the misconceptions_df has MisconceptionText or MisconceptionName\n",
    "misconception_text_col = 'MisconceptionName'  # Use the column name from your data\n",
    "if 'MisconceptionText' in misconceptions_df.columns:\n",
    "    misconception_text_col = 'MisconceptionText'\n",
    "\n",
    "# Preprocess misconception texts with the simple function\n",
    "misconceptions_df['ProcessedMisconceptionText'] = misconceptions_df[misconception_text_col].apply(preprocess_text_simple)\n",
    "\n",
    "# Check some examples of processed texts\n",
    "print(\"Sample processed queries from training data:\")\n",
    "for query, processed in zip(flattened_train_df['Query'].head(3), \n",
    "                          flattened_train_df['ProcessedQuery'].head(3)):\n",
    "    print(f\"Original: {query}\")\n",
    "    print(f\"Processed: {processed}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nSample processed misconception texts:\")\n",
    "for original, processed in zip(misconceptions_df[misconception_text_col].head(3), \n",
    "                             misconceptions_df['ProcessedMisconceptionText'].head(3)):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Processed: {processed}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a0fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_splits(df, n_splits=5, method='group'):\n",
    "    \"\"\"\n",
    "    Create validation splits for cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        n_splits: Number of splits to create\n",
    "        method: Method to use for splitting ('group' or 'stratified')\n",
    "    \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    if method == 'group':\n",
    "        # Use GroupKFold to ensure related items stay together\n",
    "        cv = GroupKFold(n_splits=n_splits)\n",
    "        splits = list(cv.split(df, groups=df['MisconceptionId']))\n",
    "    elif method == 'stratified':\n",
    "        # Use StratifiedKFold to maintain the distribution of misconceptions\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = list(cv.split(df, df['MisconceptionId']))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown split method: {method}\")\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df723505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_unseen_split(df, unseen_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a custom train/validation split where some misconceptions are completely held out\n",
    "    from the training set to simulate \"unseen\" misconceptions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        unseen_ratio: Proportion of misconceptions to hold out\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_idx, valid_idx: Indices for training and validation sets\n",
    "    \"\"\"\n",
    "    # Get unique misconceptions\n",
    "    unique_misconceptions = df['MisconceptionId'].unique()\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Randomly select misconceptions to hold out\n",
    "    n_unseen = int(len(unique_misconceptions) * unseen_ratio)\n",
    "    unseen_misconceptions = np.random.choice(unique_misconceptions, size=n_unseen, replace=False)\n",
    "    \n",
    "    # Create train and validation indices\n",
    "    train_idx = df[~df['MisconceptionId'].isin(unseen_misconceptions)].index\n",
    "    valid_idx = df[df['MisconceptionId'].isin(unseen_misconceptions)].index\n",
    "    \n",
    "    return train_idx, valid_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation splits functions\n",
    "def create_validation_splits(df, n_splits=5, method='group'):\n",
    "    \"\"\"\n",
    "    Create validation splits for cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        n_splits: Number of splits to create\n",
    "        method: Method to use for splitting ('group' or 'stratified')\n",
    "    \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    if 'MisconceptionId' not in df.columns:\n",
    "        print(\"Column 'MisconceptionId' not found in dataframe\")\n",
    "        return []\n",
    "        \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Check for and handle NaN values in the grouping column\n",
    "    if df_copy['MisconceptionId'].isna().any():\n",
    "        print(f\"Found {df_copy['MisconceptionId'].isna().sum()} NaN values in MisconceptionId. Filling with unique identifier.\")\n",
    "        df_copy['MisconceptionId'] = df_copy['MisconceptionId'].fillna(-999)  # Use a value that doesn't exist in your data\n",
    "    \n",
    "    if method == 'group':\n",
    "        # Use GroupKFold to ensure related items stay together\n",
    "        cv = GroupKFold(n_splits=n_splits)\n",
    "        splits = list(cv.split(df_copy, groups=df_copy['MisconceptionId']))\n",
    "    elif method == 'stratified':\n",
    "        # Use StratifiedKFold to maintain the distribution\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        splits = list(cv.split(df_copy, df_copy['MisconceptionId']))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown split method: {method}\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def create_custom_unseen_split(df, unseen_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a custom train/validation split where some misconceptions are completely held out.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        unseen_ratio: Proportion of misconceptions to hold out\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_idx, valid_idx: Indices for training and validation sets\n",
    "    \"\"\"\n",
    "    if 'MisconceptionId' not in df.columns:\n",
    "        print(\"Column 'MisconceptionId' not found in dataframe\")\n",
    "        return [], []\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Handle NaN values in the ID column\n",
    "    if df_copy['MisconceptionId'].isna().any():\n",
    "        print(f\"Found {df_copy['MisconceptionId'].isna().sum()} NaN values in MisconceptionId. Filling with unique identifier.\")\n",
    "        df_copy['MisconceptionId'] = df_copy['MisconceptionId'].fillna(-999)  # Use a value that doesn't exist in your data\n",
    "    \n",
    "    # Get unique values (excluding any filled NaN values)\n",
    "    unique_values = df_copy[df_copy['MisconceptionId'] != -999]['MisconceptionId'].unique()\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Randomly select values to hold out\n",
    "    n_unseen = int(len(unique_values) * unseen_ratio)\n",
    "    unseen_values = np.random.choice(unique_values, size=n_unseen, replace=False)\n",
    "    \n",
    "    # Create train and validation indices\n",
    "    train_idx = df[~df['MisconceptionId'].isin(unseen_values)].index\n",
    "    valid_idx = df[df['MisconceptionId'].isin(unseen_values)].index\n",
    "    \n",
    "    return train_idx, valid_idx\n",
    "\n",
    "# Now use the flattened_train_df instead of train_with_questions\n",
    "# Create standard cross-validation splits\n",
    "group_splits = create_validation_splits(flattened_train_df, method='group')\n",
    "stratified_splits = create_validation_splits(flattened_train_df, method='stratified')\n",
    "\n",
    "# Create a custom split with unseen misconceptions\n",
    "unseen_train_idx, unseen_valid_idx = create_custom_unseen_split(flattened_train_df, unseen_ratio=0.2)\n",
    "\n",
    "# Analyze the splits\n",
    "print(\"Standard GroupKFold splits:\")\n",
    "for fold, (train_idx, valid_idx) in enumerate(group_splits):\n",
    "    train_misconceptions = set(flattened_train_df.iloc[train_idx]['MisconceptionId'])\n",
    "    valid_misconceptions = set(flattened_train_df.iloc[valid_idx]['MisconceptionId'])\n",
    "    unseen_misconceptions = valid_misconceptions - train_misconceptions\n",
    "    \n",
    "    print(f\"Fold {fold+1}:\")\n",
    "    print(f\"  Train set size: {len(train_idx)}\")\n",
    "    print(f\"  Validation set size: {len(valid_idx)}\")\n",
    "    print(f\"  Number of unseen misconceptions in validation: {len(unseen_misconceptions)}\")\n",
    "    print(f\"  Proportion of unseen misconceptions: {len(unseen_misconceptions) / len(valid_misconceptions) if len(valid_misconceptions) > 0 else 0:.2f}\")\n",
    "\n",
    "print(\"\\nCustom split with unseen misconceptions:\")\n",
    "train_misconceptions = set(flattened_train_df.iloc[unseen_train_idx]['MisconceptionId'])\n",
    "valid_misconceptions = set(flattened_train_df.iloc[unseen_valid_idx]['MisconceptionId'])\n",
    "unseen_misconceptions = valid_misconceptions - train_misconceptions\n",
    "\n",
    "print(f\"Train set size: {len(unseen_train_idx)}\")\n",
    "print(f\"Validation set size: {len(unseen_valid_idx)}\")\n",
    "print(f\"Number of unseen misconceptions in validation: {len(unseen_misconceptions)}\")\n",
    "print(f\"Proportion of unseen misconceptions: {len(unseen_misconceptions) / len(valid_misconceptions) if len(valid_misconceptions) > 0 else 0:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data with the necessary columns\n",
    "preprocessed_train = flattened_train_df[['QuestionId', 'MisconceptionId', 'Query', 'ProcessedQuery']]\n",
    "\n",
    "# Prepare the test data\n",
    "preprocessed_test = test_df[['QuestionId', 'Query', 'ProcessedQuery']]\n",
    "\n",
    "# Check if misconceptions_df has the expected column names and adapt if needed\n",
    "misconception_text_col = 'MisconceptionName'  # Default column name in your data\n",
    "if 'MisconceptionText' in misconceptions_df.columns:\n",
    "    misconception_text_col = 'MisconceptionText'\n",
    "\n",
    "# Create a new column name for consistency\n",
    "if 'ProcessedMisconceptionText' not in misconceptions_df.columns:\n",
    "    # If we haven't preprocessed it yet, do it now\n",
    "    misconceptions_df['ProcessedMisconceptionText'] = misconceptions_df[misconception_text_col].apply(preprocess_text_simple)\n",
    "\n",
    "# Prepare the misconceptions data\n",
    "necessary_columns = ['MisconceptionId', misconception_text_col, 'ProcessedMisconceptionText']\n",
    "preprocessed_misconceptions = misconceptions_df[necessary_columns].copy()\n",
    "\n",
    "# Rename the text column for consistency if needed\n",
    "if misconception_text_col != 'MisconceptionText':\n",
    "    preprocessed_misconceptions = preprocessed_misconceptions.rename(columns={misconception_text_col: 'MisconceptionText'})\n",
    "\n",
    "# Check the shapes of the final datasets\n",
    "print(f\"Preprocessed train dataset shape: {preprocessed_train.shape}\")\n",
    "print(f\"Preprocessed test dataset shape: {preprocessed_test.shape}\")\n",
    "print(f\"Preprocessed misconceptions dataset shape: {preprocessed_misconceptions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from misconception IDs to numerical indices\n",
    "misconception_to_idx = {mid: idx for idx, mid in enumerate(misconceptions_df['MisconceptionId'].unique())}\n",
    "idx_to_misconception = {idx: mid for mid, idx in misconception_to_idx.items()}\n",
    "\n",
    "# Add encoded misconception IDs to the dataframes\n",
    "preprocessed_train['MisconceptionIdx'] = preprocessed_train['MisconceptionId'].map(misconception_to_idx)\n",
    "preprocessed_misconceptions['MisconceptionIdx'] = preprocessed_misconceptions['MisconceptionId'].map(misconception_to_idx)\n",
    "\n",
    "# Check the encodings\n",
    "print(\"Sample misconception encodings:\")\n",
    "print(preprocessed_misconceptions[['MisconceptionId', 'MisconceptionIdx']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(train_df, misconceptions_df, n_negatives=5, seed=42):\n",
    "    \"\"\"\n",
    "    Create training pairs with positive and negative examples.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Preprocessed training dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        n_negatives: Number of negative examples per positive\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with query, positive misconception, and negative misconceptions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get all unique misconception IDs\n",
    "    all_misconception_ids = misconceptions_df['MisconceptionId'].unique()\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    \n",
    "    # Group by QuestionId to keep track of positive misconceptions\n",
    "    for question_id, group in train_df.groupby('QuestionId'):\n",
    "        # Get the positive misconceptions for this question\n",
    "        positive_misconceptions = group['MisconceptionId'].tolist()\n",
    "        \n",
    "        # Get the query for this question (should be the same for all rows)\n",
    "        query = group['Query'].iloc[0]\n",
    "        processed_query = group['ProcessedQuery'].iloc[0]\n",
    "        \n",
    "        # For each positive misconception, sample negative misconceptions\n",
    "        for pos_mid in positive_misconceptions:\n",
    "            # Get the processed text for the positive misconception\n",
    "            pos_misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == pos_mid]['MisconceptionText'].iloc[0]\n",
    "            pos_processed_text = misconceptions_df[misconceptions_df['MisconceptionId'] == pos_mid]['ProcessedMisconceptionText'].iloc[0]\n",
    "            \n",
    "            # Sample negative misconceptions (those not positive for this question)\n",
    "            negative_misconception_ids = np.random.choice(\n",
    "                [mid for mid in all_misconception_ids if mid not in positive_misconceptions],\n",
    "                size=min(n_negatives, len(all_misconception_ids) - len(positive_misconceptions)),\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            # Get the text for the negative misconceptions\n",
    "            negative_misconceptions = []\n",
    "            for neg_mid in negative_misconception_ids:\n",
    "                neg_misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == neg_mid]['MisconceptionText'].iloc[0]\n",
    "                neg_processed_text = misconceptions_df[misconceptions_df['MisconceptionId'] == neg_mid]['ProcessedMisconceptionText'].iloc[0]\n",
    "                \n",
    "                negative_misconceptions.append({\n",
    "                    'MisconceptionId': neg_mid,\n",
    "                    'MisconceptionText': neg_misconception_text,\n",
    "                    'ProcessedMisconceptionText': neg_processed_text\n",
    "                })\n",
    "            \n",
    "            # Add the data point\n",
    "            data.append({\n",
    "                'QuestionId': question_id,\n",
    "                'Query': query,\n",
    "                'ProcessedQuery': processed_query,\n",
    "                'PositiveMisconceptionId': pos_mid,\n",
    "                'PositiveMisconceptionText': pos_misconception_text,\n",
    "                'ProcessedPositiveMisconceptionText': pos_processed_text,\n",
    "                'NegativeMisconceptions': negative_misconceptions\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b38032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training pairs with positive and negative examples\n",
    "training_pairs = create_training_pairs(preprocessed_train, preprocessed_misconceptions, n_negatives=5)\n",
    "\n",
    "# Check the structure of the training pairs\n",
    "print(f\"Training pairs shape: {training_pairs.shape}\")\n",
    "print(\"\\nSample training pair:\")\n",
    "sample_pair = training_pairs.iloc[0]\n",
    "print(f\"Question ID: {sample_pair['QuestionId']}\")\n",
    "print(f\"Query: {sample_pair['Query']}\")\n",
    "print(f\"Positive Misconception ID: {sample_pair['PositiveMisconceptionId']}\")\n",
    "print(f\"Positive Misconception Text: {sample_pair['PositiveMisconceptionText']}\")\n",
    "print(\"\\nNegative Misconceptions:\")\n",
    "for i, neg_misc in enumerate(sample_pair['NegativeMisconceptions']):\n",
    "    print(f\"  {i+1}. ID: {neg_misc['MisconceptionId']}, Text: {neg_misc['MisconceptionText']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301960f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final datasets for saving\n",
    "train_data = {\n",
    "    'preprocessed_train': preprocessed_train,\n",
    "    'preprocessed_test': preprocessed_test,\n",
    "    'preprocessed_misconceptions': preprocessed_misconceptions,\n",
    "    'training_pairs': training_pairs,\n",
    "    'cv_splits': {\n",
    "        'group_splits': group_splits,\n",
    "        'stratified_splits': stratified_splits,\n",
    "        'unseen_split': (unseen_train_idx, unseen_valid_idx)\n",
    "    },\n",
    "    'encodings': {\n",
    "        'misconception_to_idx': misconception_to_idx,\n",
    "        'idx_to_misconception': idx_to_misconception\n",
    "    }\n",
    "}\n",
    "\n",
    "# In a real environment, you'd save these datasets to disk\n",
    "# For Kaggle, you might save to a pickle file\n",
    "import pickle\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "print(\"Preprocessed data saved for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "preprocessed_train = train_data['preprocessed_train']\n",
    "preprocessed_test = train_data['preprocessed_test']\n",
    "preprocessed_misconceptions = train_data['preprocessed_misconceptions']\n",
    "training_pairs = train_data['training_pairs']\n",
    "cv_splits = train_data['cv_splits']\n",
    "misconception_to_idx = train_data['encodings']['misconception_to_idx']\n",
    "idx_to_misconception = train_data['encodings']['idx_to_misconception']\n",
    "\n",
    "print(f\"Loaded preprocessed train data with {len(preprocessed_train)} rows\")\n",
    "print(f\"Loaded preprocessed test data with {len(preprocessed_test)} rows\")\n",
    "print(f\"Loaded preprocessed misconceptions data with {len(preprocessed_misconceptions)} rows\")\n",
    "print(f\"Loaded training pairs data with {len(training_pairs)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def tfidf_similarity_baseline(train_df, test_df, misconceptions_df, top_k=25):\n",
    "    \"\"\"\n",
    "    Create a simple baseline using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Preprocessed training dataframe\n",
    "        test_df: Preprocessed test dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        top_k: Number of top misconceptions to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions for test queries\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    \n",
    "    # Fit and transform on all text (queries and misconceptions)\n",
    "    all_text = list(train_df['ProcessedQuery']) + list(test_df['ProcessedQuery']) + list(misconceptions_df['ProcessedMisconceptionText'])\n",
    "    tfidf.fit(all_text)\n",
    "    \n",
    "    # Transform misconceptions\n",
    "    misconception_vectors = tfidf.transform(misconceptions_df['ProcessedMisconceptionText'])\n",
    "    \n",
    "    # Transform test queries\n",
    "    test_vectors = tfidf.transform(test_df['ProcessedQuery'])\n",
    "    \n",
    "    # Compute similarity between test queries and misconceptions\n",
    "    similarity_matrix = cosine_similarity(test_vectors, misconception_vectors)\n",
    "    \n",
    "    # Get top-k misconceptions for each test query\n",
    "    predictions = {}\n",
    "    for i, question_id in enumerate(test_df['QuestionId']):\n",
    "        # Get similarity scores for this query\n",
    "        scores = similarity_matrix[i]\n",
    "        \n",
    "        # Get indices of top-k misconceptions\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Get misconception IDs and scores\n",
    "        top_misconceptions = [misconceptions_df.iloc[idx]['MisconceptionId'] for idx in top_indices]\n",
    "        top_scores = [scores[idx] for idx in top_indices]\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions[question_id] = list(zip(top_misconceptions, top_scores))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324834ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map_at_k(predictions, ground_truth, k=25):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at k.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        k: k for MAP@k\n",
    "        \n",
    "    Returns:\n",
    "        MAP@k score\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Calculate AP@k for each question\n",
    "    ap_scores = []\n",
    "    for question_id, true_misconceptions in gt_by_question.items():\n",
    "        if question_id not in predictions:\n",
    "            ap_scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Get predicted misconceptions\n",
    "        pred_misconceptions = [mid for mid, _ in predictions[question_id][:k]]\n",
    "        \n",
    "        # Calculate precision at each position\n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        \n",
    "        for i, mid in enumerate(pred_misconceptions):\n",
    "            if mid in true_misconceptions:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "        \n",
    "        # Calculate AP@k\n",
    "        if len(precisions) > 0:\n",
    "            ap = sum(precisions) / min(len(true_misconceptions), k)\n",
    "        else:\n",
    "            ap = 0\n",
    "        \n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    # Calculate MAP@k\n",
    "    return np.mean(ap_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a validation split\n",
    "unseen_train_idx, unseen_valid_idx = cv_splits['unseen_split']\n",
    "train_subset = preprocessed_train.iloc[unseen_train_idx]\n",
    "valid_subset = preprocessed_train.iloc[unseen_valid_idx]\n",
    "\n",
    "# Run baseline on validation data\n",
    "baseline_predictions = tfidf_similarity_baseline(train_subset, valid_subset, preprocessed_misconceptions)\n",
    "\n",
    "# Calculate MAP@25\n",
    "baseline_map = calculate_map_at_k(baseline_predictions, valid_subset, k=25)\n",
    "print(f\"Baseline TF-IDF + Cosine Similarity MAP@25: {baseline_map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03839a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedder:\n",
    "    \"\"\"\n",
    "    Class to generate embeddings using a pre-trained transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to get sentence embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def generate_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded_input = self.tokenizer(\n",
    "                batch_texts, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=512, \n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(**encoded_input)\n",
    "            \n",
    "            # Pool embeddings\n",
    "            batch_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "            embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa629ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisconceptionPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training with pairs of queries and misconceptions.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs_df, tokenizer, max_length=512):\n",
    "        self.pairs_df = pairs_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs_df.iloc[idx]\n",
    "        \n",
    "        # Get query text and positive misconception text\n",
    "        query_text = pair['ProcessedQuery']\n",
    "        pos_misc_text = pair['ProcessedPositiveMisconceptionText']\n",
    "        \n",
    "        # Randomly select one negative misconception\n",
    "        neg_misconceptions = pair['NegativeMisconceptions']\n",
    "        if len(neg_misconceptions) > 0:\n",
    "            neg_misc = random.choice(neg_misconceptions)\n",
    "            neg_misc_text = neg_misc['ProcessedMisconceptionText']\n",
    "        else:\n",
    "            # If no negative misconceptions available, use the positive one but with a label of 0\n",
    "            neg_misc_text = pos_misc_text\n",
    "        \n",
    "        # Tokenize texts\n",
    "        query_encoding = self.tokenizer(\n",
    "            query_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        pos_encoding = self.tokenizer(\n",
    "            pos_misc_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        neg_encoding = self.tokenizer(\n",
    "            neg_misc_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoding['input_ids'].squeeze(),\n",
    "            'query_attention_mask': query_encoding['attention_mask'].squeeze(),\n",
    "            'pos_input_ids': pos_encoding['input_ids'].squeeze(),\n",
    "            'pos_attention_mask': pos_encoding['attention_mask'].squeeze(),\n",
    "            'neg_input_ids': neg_encoding['input_ids'].squeeze(),\n",
    "            'neg_attention_mask': neg_encoding['attention_mask'].squeeze(),\n",
    "            'question_id': pair['QuestionId'],\n",
    "            'pos_misconception_id': pair['PositiveMisconceptionId']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5993d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual encoder model for learning query and misconception embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\", embedding_dim=768):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.query_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.misconception_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Projection layers\n",
    "        self.query_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.misconception_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to get sentence embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def encode_query(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Encode query inputs.\n",
    "        \"\"\"\n",
    "        outputs = self.query_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return self.query_proj(embeddings)\n",
    "    \n",
    "    def encode_misconception(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Encode misconception inputs.\n",
    "        \"\"\"\n",
    "        outputs = self.misconception_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return self.misconception_proj(embeddings)\n",
    "    \n",
    "    def forward(self, query_input_ids, query_attention_mask, misc_input_ids, misc_attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass to calculate similarity between query and misconception.\n",
    "        \"\"\"\n",
    "        query_embeddings = self.encode_query(query_input_ids, query_attention_mask)\n",
    "        misc_embeddings = self.encode_misconception(misc_input_ids, misc_attention_mask)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "        misc_embeddings = F.normalize(misc_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        return torch.matmul(query_embeddings, misc_embeddings.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5077061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss for training the dual encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, query_embeddings, pos_embeddings, neg_embeddings):\n",
    "        \"\"\"\n",
    "        Calculate triplet loss.\n",
    "        \n",
    "        Args:\n",
    "            query_embeddings: Embeddings of queries\n",
    "            pos_embeddings: Embeddings of positive misconceptions\n",
    "            neg_embeddings: Embeddings of negative misconceptions\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "        pos_embeddings = F.normalize(pos_embeddings, p=2, dim=1)\n",
    "        neg_embeddings = F.normalize(neg_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        pos_similarity = torch.sum(query_embeddings * pos_embeddings, dim=1)\n",
    "        neg_similarity = torch.sum(query_embeddings * neg_embeddings, dim=1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = torch.clamp(self.margin - pos_similarity + neg_similarity, min=0)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f36f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dual_encoder(model, train_loader, val_loader=None, epochs=3, lr=2e-5, warmup_steps=0, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "    Train the dual encoder model.\n",
    "    \n",
    "    Args:\n",
    "        model: DualEncoder model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data (optional)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        warmup_steps: Number of warmup steps for learning rate scheduler\n",
    "        weight_decay: Weight decay for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set up loss function\n",
    "    triplet_loss = TripletLoss(margin=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            pos_input_ids = batch['pos_input_ids'].to(device)\n",
    "            pos_attention_mask = batch['pos_attention_mask'].to(device)\n",
    "            neg_input_ids = batch['neg_input_ids'].to(device)\n",
    "            neg_attention_mask = batch['neg_attention_mask'].to(device)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            query_embeddings = model.encode_query(query_input_ids, query_attention_mask)\n",
    "            pos_embeddings = model.encode_misconception(pos_input_ids, pos_attention_mask)\n",
    "            neg_embeddings = model.encode_misconception(neg_input_ids, neg_attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = triplet_loss(query_embeddings, pos_embeddings, neg_embeddings)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    # Move batch to device\n",
    "                    query_input_ids = batch['query_input_ids'].to(device)\n",
    "                    query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "                    pos_input_ids = batch['pos_input_ids'].to(device)\n",
    "                    pos_attention_mask = batch['pos_attention_mask'].to(device)\n",
    "                    neg_input_ids = batch['neg_input_ids'].to(device)\n",
    "                    neg_attention_mask = batch['neg_attention_mask'].to(device)\n",
    "                    \n",
    "                    # Generate embeddings\n",
    "                    query_embeddings = model.encode_query(query_input_ids, query_attention_mask)\n",
    "                    pos_embeddings = model.encode_misconception(pos_input_ids, pos_attention_mask)\n",
    "                    neg_embeddings = model.encode_misconception(neg_input_ids, neg_attention_mask)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = triplet_loss(query_embeddings, pos_embeddings, neg_embeddings)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36491004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_with_dual_encoder(model, test_df, misconceptions_df, batch_size=32, top_k=25):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained dual encoder model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DualEncoder model\n",
    "        test_df: Preprocessed test dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        batch_size: Batch size for generating embeddings\n",
    "        top_k: Number of top misconceptions to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions for test queries\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.query_encoder.config._name_or_path)\n",
    "    \n",
    "    # Generate query embeddings\n",
    "    query_embeddings = []\n",
    "    for i in tqdm(range(0, len(test_df), batch_size), desc=\"Generating query embeddings\"):\n",
    "        batch_texts = test_df['ProcessedQuery'].iloc[i:i+batch_size].tolist()\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode_query(\n",
    "                encoded_input['input_ids'], \n",
    "                encoded_input['attention_mask']\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            query_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    query_embeddings = np.vstack(query_embeddings)\n",
    "    \n",
    "    # Generate misconception embeddings\n",
    "    misconception_embeddings = []\n",
    "    for i in tqdm(range(0, len(misconceptions_df), batch_size), desc=\"Generating misconception embeddings\"):\n",
    "        batch_texts = misconceptions_df['ProcessedMisconceptionText'].iloc[i:i+batch_size].tolist()\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode_misconception(\n",
    "                encoded_input['input_ids'], \n",
    "                encoded_input['attention_mask']\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            misconception_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    misconception_embeddings = np.vstack(misconception_embeddings)\n",
    "    \n",
    "    # Compute similarity between queries and misconceptions\n",
    "    similarity_matrix = np.matmul(query_embeddings, misconception_embeddings.T)\n",
    "    \n",
    "    # Get top-k misconceptions for each query\n",
    "    predictions = {}\n",
    "    for i, question_id in enumerate(test_df['QuestionId']):\n",
    "        # Get similarity scores for this query\n",
    "        scores = similarity_matrix[i]\n",
    "        \n",
    "        # Get indices of top-k misconceptions\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Get misconception IDs and scores\n",
    "        top_misconceptions = [misconceptions_df.iloc[idx]['MisconceptionId'] for idx in top_indices]\n",
    "        top_scores = [float(scores[idx]) for idx in top_indices]\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions[question_id] = list(zip(top_misconceptions, top_scores))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "unseen_train_idx, unseen_valid_idx = cv_splits['unseen_split']\n",
    "train_subset = preprocessed_train.iloc[unseen_train_idx]\n",
    "valid_subset = preprocessed_train.iloc[unseen_valid_idx]\n",
    "\n",
    "# Get training pairs from train subset\n",
    "train_pairs = training_pairs[training_pairs['QuestionId'].isin(train_subset['QuestionId'])]\n",
    "valid_pairs = training_pairs[training_pairs['QuestionId'].isin(valid_subset['QuestionId'])]\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DualEncoder(model_name=model_name)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MisconceptionPairDataset(train_pairs, tokenizer)\n",
    "valid_dataset = MisconceptionPairDataset(valid_pairs, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train_dual_encoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=valid_loader,\n",
    "    epochs=3,\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Generate predictions for validation set\n",
    "valid_predictions = generate_predictions_with_dual_encoder(\n",
    "    trained_model,\n",
    "    valid_subset,\n",
    "    preprocessed_misconceptions,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Calculate MAP@25\n",
    "dual_encoder_map = calculate_map_at_k(valid_predictions, valid_subset, k=25)\n",
    "print(f\"Dual Encoder MAP@25: {dual_encoder_map:.4f}\")\n",
    "print(f\"Improvement over baseline: {dual_encoder_map - baseline_map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(predictions, ground_truth, misconceptions_df, n_examples=5):\n",
    "    \"\"\"\n",
    "    Analyze predictions for a few examples.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        misconceptions_df: DataFrame with misconception information\n",
    "        n_examples: Number of examples to analyze\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Select random examples\n",
    "    question_ids = random.sample(list(gt_by_question.keys()), min(n_examples, len(gt_by_question)))\n",
    "    \n",
    "    for i, question_id in enumerate(question_ids):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        \n",
    "        # Get question text\n",
    "        question_text = ground_truth[ground_truth['QuestionId'] == question_id]['Query'].iloc[0]\n",
    "        print(f\"Question: {question_text}\")\n",
    "        \n",
    "        # Get true misconceptions\n",
    "        true_misconceptions = gt_by_question[question_id]\n",
    "        print(\"\\nTrue Misconceptions:\")\n",
    "        for j, mid in enumerate(true_misconceptions):\n",
    "            misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "            print(f\"  {j+1}. {mid}: {misconception_text}\")\n",
    "        \n",
    "        # Get predicted misconceptions\n",
    "        if question_id in predictions:\n",
    "            print(\"\\nTop 5 Predicted Misconceptions:\")\n",
    "            for j, (mid, score) in enumerate(predictions[question_id][:5]):\n",
    "                misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "                is_correct = mid in true_misconceptions\n",
    "                print(f\"  {j+1}. {mid}: {misconception_text} (Score: {score:.4f}, Correct: {is_correct})\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_predictions(valid_predictions, valid_subset, preprocessed_misconceptions, n_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d615130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "test_predictions = generate_predictions_with_dual_encoder(\n",
    "    trained_model,\n",
    "    preprocessed_test,\n",
    "    preprocessed_misconceptions,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Format predictions for submission\n",
    "def format_predictions_for_submission(predictions):\n",
    "    \"\"\"\n",
    "    Format predictions for submission.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns QuestionId, MisconceptionIds\n",
    "    \"\"\"\n",
    "    submission_data = []\n",
    "    \n",
    "    for question_id, preds in predictions.items():\n",
    "        # Get misconception IDs\n",
    "        misconception_ids = [mid for mid, _ in preds]\n",
    "        \n",
    "        # Convert to string\n",
    "        misconception_ids_str = ' '.join(map(str, misconception_ids))\n",
    "        \n",
    "        # Add to submission data\n",
    "        submission_data.append({\n",
    "            'QuestionId': question_id,\n",
    "            'MisconceptionIds': misconception_ids_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(submission_data)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = format_predictions_for_submission(test_predictions)\n",
    "print(f\"Created submission with {len(submission_df)} rows\")\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from previous steps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def calculate_map_at_k(predictions, ground_truth, k=25):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at k.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        k: k for MAP@k\n",
    "        \n",
    "    Returns:\n",
    "        MAP@k score\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Calculate AP@k for each question\n",
    "    ap_scores = []\n",
    "    for question_id, true_misconceptions in gt_by_question.items():\n",
    "        if question_id not in predictions:\n",
    "            ap_scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Get predicted misconceptions\n",
    "        pred_misconceptions = [mid for mid, _ in predictions[question_id][:k]]\n",
    "        \n",
    "        # Calculate precision at each position\n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        \n",
    "        for i, mid in enumerate(pred_misconceptions):\n",
    "            if mid in true_misconceptions:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "        \n",
    "        # Calculate AP@k\n",
    "        if len(precisions) > 0:\n",
    "            ap = sum(precisions) / min(len(true_misconceptions), k)\n",
    "        else:\n",
    "            ap = 0\n",
    "        \n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    # Calculate MAP@k\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "def tfidf_similarity_baseline(train_df, test_df, misconceptions_df, top_k=25):\n",
    "    \"\"\"\n",
    "    Create a simple baseline using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Preprocessed training dataframe\n",
    "        test_df: Preprocessed test dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        top_k: Number of top misconceptions to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions for test queries\n",
    "    \"\"\"\n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(max_features=5000)\n",
    "    \n",
    "    # Fit and transform on all text (queries and misconceptions)\n",
    "    all_text = list(train_df['ProcessedQuery']) + list(test_df['ProcessedQuery']) + list(misconceptions_df['ProcessedMisconceptionText'])\n",
    "    tfidf.fit(all_text)\n",
    "    \n",
    "    # Transform misconceptions\n",
    "    misconception_vectors = tfidf.transform(misconceptions_df['ProcessedMisconceptionText'])\n",
    "    \n",
    "    # Transform test queries\n",
    "    test_vectors = tfidf.transform(test_df['ProcessedQuery'])\n",
    "    \n",
    "    # Compute similarity between test queries and misconceptions\n",
    "    similarity_matrix = cosine_similarity(test_vectors, misconception_vectors)\n",
    "    \n",
    "    # Get top-k misconceptions for each test query\n",
    "    predictions = {}\n",
    "    for i, question_id in enumerate(test_df['QuestionId']):\n",
    "        # Get similarity scores for this query\n",
    "        scores = similarity_matrix[i]\n",
    "        \n",
    "        # Get indices of top-k misconceptions\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Get misconception IDs and scores\n",
    "        top_misconceptions = [misconceptions_df.iloc[idx]['MisconceptionId'] for idx in top_indices]\n",
    "        top_scores = [scores[idx] for idx in top_indices]\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions[question_id] = list(zip(top_misconceptions, top_scores))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Get validation split from CV splits\n",
    "unseen_train_idx, unseen_valid_idx = cv_splits['unseen_split']\n",
    "train_subset = preprocessed_train.iloc[unseen_train_idx]\n",
    "valid_subset = preprocessed_train.iloc[unseen_valid_idx]\n",
    "\n",
    "# Create baseline predictions for validation and test sets\n",
    "print(\"Generating baseline predictions for validation set...\")\n",
    "valid_predictions = tfidf_similarity_baseline(train_subset, valid_subset, preprocessed_misconceptions)\n",
    "baseline_map = calculate_map_at_k(valid_predictions, valid_subset, k=25)\n",
    "print(f\"Baseline TF-IDF + Cosine Similarity MAP@25: {baseline_map:.4f}\")\n",
    "\n",
    "print(\"Generating baseline predictions for test set...\")\n",
    "test_predictions = tfidf_similarity_baseline(train_subset, preprocessed_test, preprocessed_misconceptions)\n",
    "\n",
    "# Save predictions to file\n",
    "retrieval_predictions = {\n",
    "    'valid_predictions': valid_predictions,\n",
    "    'test_predictions': test_predictions\n",
    "}\n",
    "\n",
    "with open('retrieval_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(retrieval_predictions, f)\n",
    "\n",
    "print(\"Retrieval predictions saved to 'retrieval_predictions.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed25f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "preprocessed_train = train_data['preprocessed_train']\n",
    "preprocessed_test = train_data['preprocessed_test']\n",
    "preprocessed_misconceptions = train_data['preprocessed_misconceptions']\n",
    "training_pairs = train_data['training_pairs']\n",
    "cv_splits = train_data['cv_splits']\n",
    "misconception_to_idx = train_data['encodings']['misconception_to_idx']\n",
    "idx_to_misconception = train_data['encodings']['idx_to_misconception']\n",
    "\n",
    "# Load predictions from retrieval model\n",
    "with open('retrieval_predictions.pkl', 'rb') as f:\n",
    "    retrieval_predictions = pickle.load(f)\n",
    "\n",
    "valid_predictions = retrieval_predictions['valid_predictions']\n",
    "test_predictions = retrieval_predictions['test_predictions']\n",
    "\n",
    "print(f\"Loaded preprocessed train data with {len(preprocessed_train)} rows\")\n",
    "print(f\"Loaded preprocessed test data with {len(preprocessed_test)} rows\")\n",
    "print(f\"Loaded retrieval predictions for {len(valid_predictions)} validation questions and {len(test_predictions)} test questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_reranking_data(query_df, misconceptions_df, retrieval_predictions, ground_truth=None, top_k=25, n_negatives=5):\n",
    "    \"\"\"\n",
    "    Prepare data for training/evaluating a reranking model.\n",
    "    \n",
    "    Args:\n",
    "        query_df: DataFrame with queries\n",
    "        misconceptions_df: DataFrame with misconceptions\n",
    "        retrieval_predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns (for training data)\n",
    "        top_k: Number of top misconceptions to use from retrieval predictions\n",
    "        n_negatives: Number of negative examples per positive (for training data)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with query-misconception pairs and labels\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Create a lookup for ground truth misconceptions\n",
    "    gt_by_question = {}\n",
    "    if ground_truth is not None:\n",
    "        for _, row in ground_truth.iterrows():\n",
    "            question_id = row['QuestionId']\n",
    "            misconception_id = row['MisconceptionId']\n",
    "            \n",
    "            if question_id not in gt_by_question:\n",
    "                gt_by_question[question_id] = []\n",
    "            \n",
    "            gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Process each query\n",
    "    for question_id, query_row in query_df.iterrows():\n",
    "        question_id = query_row['QuestionId']\n",
    "        \n",
    "        # Skip if no retrieval predictions for this question\n",
    "        if question_id not in retrieval_predictions:\n",
    "            continue\n",
    "        \n",
    "        # Get query text\n",
    "        query_text = query_row['Query']\n",
    "        processed_query = query_row['ProcessedQuery']\n",
    "        \n",
    "        # Get top-k misconceptions from retrieval predictions\n",
    "        top_misconceptions = retrieval_predictions[question_id][:top_k]\n",
    "        \n",
    "        if ground_truth is not None:\n",
    "            # This is for training data - create positive and negative examples\n",
    "            if question_id in gt_by_question:\n",
    "                true_misconceptions = gt_by_question[question_id]\n",
    "                \n",
    "                # Add all retrieved misconceptions as examples\n",
    "                for misconception_id, score in top_misconceptions:\n",
    "                    # Get misconception text\n",
    "                    misconception_row = misconceptions_df[misconceptions_df['MisconceptionId'] == misconception_id].iloc[0]\n",
    "                    misconception_text = misconception_row['MisconceptionText']\n",
    "                    processed_misconception = misconception_row['ProcessedMisconceptionText']\n",
    "                    \n",
    "                    # Determine label (1 for positive, 0 for negative)\n",
    "                    label = 1 if misconception_id in true_misconceptions else 0\n",
    "                    \n",
    "                    # Add to data\n",
    "                    data.append({\n",
    "                        'QuestionId': question_id,\n",
    "                        'MisconceptionId': misconception_id,\n",
    "                        'Query': query_text,\n",
    "                        'ProcessedQuery': processed_query,\n",
    "                        'MisconceptionText': misconception_text,\n",
    "                        'ProcessedMisconceptionText': processed_misconception,\n",
    "                        'RetrievalScore': score,\n",
    "                        'Label': label\n",
    "                    })\n",
    "                \n",
    "                # If we don't have enough positive examples, add some from ground truth\n",
    "                retrieved_positives = [mid for mid, _ in top_misconceptions if mid in true_misconceptions]\n",
    "                missing_positives = [mid for mid in true_misconceptions if mid not in retrieved_positives]\n",
    "                \n",
    "                for misconception_id in missing_positives:\n",
    "                    # Get misconception text\n",
    "                    misconception_row = misconceptions_df[misconceptions_df['MisconceptionId'] == misconception_id].iloc[0]\n",
    "                    misconception_text = misconception_row['MisconceptionText']\n",
    "                    processed_misconception = misconception_row['ProcessedMisconceptionText']\n",
    "                    \n",
    "                    # Add to data with a default retrieval score\n",
    "                    data.append({\n",
    "                        'QuestionId': question_id,\n",
    "                        'MisconceptionId': misconception_id,\n",
    "                        'Query': query_text,\n",
    "                        'ProcessedQuery': processed_query,\n",
    "                        'MisconceptionText': misconception_text,\n",
    "                        'ProcessedMisconceptionText': processed_misconception,\n",
    "                        'RetrievalScore': 0.0,  # Default score for missing examples\n",
    "                        'Label': 1\n",
    "                    })\n",
    "        else:\n",
    "            # This is for test data - just add all retrieved misconceptions\n",
    "            for misconception_id, score in top_misconceptions:\n",
    "                # Get misconception text\n",
    "                misconception_row = misconceptions_df[misconceptions_df['MisconceptionId'] == misconception_id].iloc[0]\n",
    "                misconception_text = misconception_row['MisconceptionText']\n",
    "                processed_misconception = misconception_row['ProcessedMisconceptionText']\n",
    "                \n",
    "                # Add to data (no label for test data)\n",
    "                data.append({\n",
    "                    'QuestionId': question_id,\n",
    "                    'MisconceptionId': misconception_id,\n",
    "                    'Query': query_text,\n",
    "                    'ProcessedQuery': processed_query,\n",
    "                    'MisconceptionText': misconception_text,\n",
    "                    'ProcessedMisconceptionText': processed_misconception,\n",
    "                    'RetrievalScore': score\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d095d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation set from previous cross-validation split\n",
    "unseen_train_idx, unseen_valid_idx = cv_splits['unseen_split']\n",
    "train_subset = preprocessed_train.iloc[unseen_train_idx]\n",
    "valid_subset = preprocessed_train.iloc[unseen_valid_idx]\n",
    "\n",
    "# Prepare reranking data for validation set\n",
    "valid_reranking_data = prepare_reranking_data(\n",
    "    valid_subset, \n",
    "    preprocessed_misconceptions, \n",
    "    valid_predictions, \n",
    "    ground_truth=valid_subset,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Check the data\n",
    "print(f\"Prepared reranking data with {len(valid_reranking_data)} rows\")\n",
    "print(f\"Positive examples: {valid_reranking_data['Label'].sum()}\")\n",
    "print(f\"Negative examples: {len(valid_reranking_data) - valid_reranking_data['Label'].sum()}\")\n",
    "valid_reranking_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2ae8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training a reranking model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Get query and misconception text\n",
    "        query_text = row['Query']\n",
    "        misconception_text = row['MisconceptionText']\n",
    "        \n",
    "        # Tokenize as a pair\n",
    "        encoding = self.tokenizer(\n",
    "            query_text,\n",
    "            misconception_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation='longest_first',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'token_type_ids': encoding.get('token_type_ids', torch.zeros_like(encoding['attention_mask'])).squeeze(),\n",
    "            'question_id': row['QuestionId'],\n",
    "            'misconception_id': row['MisconceptionId'],\n",
    "            'retrieval_score': torch.tensor(row['RetrievalScore'], dtype=torch.float32)\n",
    "        }\n",
    "        \n",
    "        # Add label for training data\n",
    "        if 'Label' in row:\n",
    "            item['label'] = torch.tensor(row['Label'], dtype=torch.long)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fb65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reranker(model, train_loader, val_loader=None, epochs=3, lr=2e-5, warmup_ratio=0.1, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "    Train a cross-encoder reranking model.\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained sequence classification model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data (optional)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        warmup_ratio: Ratio of warmup steps\n",
    "        weight_decay: Weight decay for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Set up loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch.get('token_type_ids', torch.zeros_like(attention_mask)).to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += len(labels)\n",
    "            \n",
    "            # Update progress bar\n",
    "            acc = train_correct / train_total\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'acc': acc})\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    # Move batch to device\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    token_type_ids = batch.get('token_type_ids', torch.zeros_like(attention_mask)).to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += len(labels)\n",
    "            \n",
    "            # Calculate average validation metrics\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = val_correct / val_total\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Train Acc: {train_acc:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_with_reranker(model, test_df, batch_size=16, top_k=25):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained reranking model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained reranking model\n",
    "        test_df: DataFrame with query-misconception pairs for testing\n",
    "        batch_size: Batch size for generating predictions\n",
    "        top_k: Number of top misconceptions to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    test_dataset = RerankerDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Generating reranking predictions\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch.get('token_type_ids', torch.zeros_like(attention_mask)).to(device)\n",
    "            question_ids = batch['question_id']\n",
    "            misconception_ids = batch['misconception_id']\n",
    "            retrieval_scores = batch['retrieval_score']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Get logits and convert to probabilities\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            positive_probs = probs[:, 1].cpu().numpy()  # Probability of positive class\n",
    "            \n",
    "            # Calculate combined scores (weighted combination of retrieval and reranking scores)\n",
    "            alpha = 0.7  # Weight for reranking scores\n",
    "            combined_scores = alpha * positive_probs + (1 - alpha) * retrieval_scores.cpu().numpy()\n",
    "            \n",
    "            # Store predictions\n",
    "            for i in range(len(question_ids)):\n",
    "                question_id = question_ids[i].item()\n",
    "                misconception_id = misconception_ids[i].item()\n",
    "                score = combined_scores[i]\n",
    "                \n",
    "                if question_id not in predictions:\n",
    "                    predictions[question_id] = []\n",
    "                \n",
    "                predictions[question_id].append((misconception_id, score))\n",
    "    \n",
    "    # Sort predictions and keep top-k for each question\n",
    "    for question_id in predictions:\n",
    "        predictions[question_id] = sorted(predictions[question_id], key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split validation data into train and validation sets for reranker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reranker_train_df, reranker_val_df = train_test_split(\n",
    "    valid_reranking_data, \n",
    "    test_size=0.2, \n",
    "    stratify=valid_reranking_data['Label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Reranker train data: {len(reranker_train_df)} rows\")\n",
    "print(f\"Reranker validation data: {len(reranker_val_df)} rows\")\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = RerankerDataset(reranker_train_df, tokenizer)\n",
    "val_dataset = RerankerDataset(reranker_val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "trained_reranker, history = train_reranker(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate reranked predictions for validation set\n",
    "reranked_valid_predictions = generate_predictions_with_reranker(\n",
    "    trained_reranker,\n",
    "    valid_reranking_data,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Calculate MAP@25 for original and reranked predictions\n",
    "def calculate_map_at_k(predictions, ground_truth, k=25):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at k.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        k: k for MAP@k\n",
    "        \n",
    "    Returns:\n",
    "        MAP@k score\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Calculate AP@k for each question\n",
    "    ap_scores = []\n",
    "    for question_id, true_misconceptions in gt_by_question.items():\n",
    "        if question_id not in predictions:\n",
    "            ap_scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Get predicted misconceptions\n",
    "        pred_misconceptions = [mid for mid, _ in predictions[question_id][:k]]\n",
    "        \n",
    "        # Calculate precision at each position\n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        \n",
    "        for i, mid in enumerate(pred_misconceptions):\n",
    "            if mid in true_misconceptions:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "        \n",
    "        # Calculate AP@k\n",
    "        if len(precisions) > 0:\n",
    "            ap = sum(precisions) / min(len(true_misconceptions), k)\n",
    "        else:\n",
    "            ap = 0\n",
    "        \n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    # Calculate MAP@k\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "original_map = calculate_map_at_k(valid_predictions, valid_subset, k=25)\n",
    "reranked_map = calculate_map_at_k(reranked_valid_predictions, valid_subset, k=25)\n",
    "\n",
    "print(f\"Original MAP@25: {original_map:.4f}\")\n",
    "print(f\"Reranked MAP@25: {reranked_map:.4f}\")\n",
    "print(f\"Improvement: {reranked_map - original_map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rankings(original_predictions, reranked_predictions, ground_truth, misconceptions_df, n_examples=3):\n",
    "    \"\"\"\n",
    "    Compare original retrieval rankings with reranked results.\n",
    "    \n",
    "    Args:\n",
    "        original_predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        reranked_predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        misconceptions_df: DataFrame with misconception information\n",
    "        n_examples: Number of examples to analyze\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Find questions with differences in rankings\n",
    "    common_questions = set(original_predictions.keys()) & set(reranked_predictions.keys()) & set(gt_by_question.keys())\n",
    "    \n",
    "    # Filter for questions where reranking improved the results\n",
    "    improved_questions = []\n",
    "    \n",
    "    for question_id in common_questions:\n",
    "        # Get true misconceptions\n",
    "        true_misconceptions = gt_by_question[question_id]\n",
    "        \n",
    "        # Get original and reranked predictions\n",
    "        original_top_k = [mid for mid, _ in original_predictions[question_id][:5]]\n",
    "        reranked_top_k = [mid for mid, _ in reranked_predictions[question_id][:5]]\n",
    "        \n",
    "        # Count correct predictions in top-5\n",
    "        original_correct = sum(1 for mid in original_top_k if mid in true_misconceptions)\n",
    "        reranked_correct = sum(1 for mid in reranked_top_k if mid in true_misconceptions)\n",
    "        \n",
    "        # Check if reranking improved results\n",
    "        if reranked_correct > original_correct:\n",
    "            improved_questions.append((question_id, reranked_correct - original_correct))\n",
    "    \n",
    "    # Sort by improvement magnitude\n",
    "    improved_questions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top examples\n",
    "    selected_questions = [qid for qid, _ in improved_questions[:n_examples]]\n",
    "    \n",
    "    # Analyze each selected question\n",
    "    for i, question_id in enumerate(selected_questions):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        \n",
    "        # Get question text\n",
    "        question_text = ground_truth[ground_truth['QuestionId'] == question_id]['Query'].iloc[0]\n",
    "        print(f\"Question: {question_text}\")\n",
    "        \n",
    "        # Get true misconceptions\n",
    "        true_misconceptions = gt_by_question[question_id]\n",
    "        print(\"\\nTrue Misconceptions:\")\n",
    "        for j, mid in enumerate(true_misconceptions):\n",
    "            misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "            print(f\"  {j+1}. {mid}: {misconception_text}\")\n",
    "        \n",
    "        # Show original rankings\n",
    "        print(\"\\nOriginal Top 5 Predictions:\")\n",
    "        for j, (mid, score) in enumerate(original_predictions[question_id][:5]):\n",
    "            misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "            is_correct = mid in true_misconceptions\n",
    "            print(f\"  {j+1}. {mid}: {misconception_text} (Score: {score:.4f}, Correct: {is_correct})\")\n",
    "        \n",
    "        # Show reranked predictions\n",
    "        print(\"\\nReranked Top 5 Predictions:\")\n",
    "        for j, (mid, score) in enumerate(reranked_predictions[question_id][:5]):\n",
    "            misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "            is_correct = mid in true_misconceptions\n",
    "            print(f\"  {j+1}. {mid}: {misconception_text} (Score: {score:.4f}, Correct: {is_correct})\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_rankings(valid_predictions, reranked_valid_predictions, valid_subset, preprocessed_misconceptions, n_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare reranking data for test set\n",
    "test_reranking_data = prepare_reranking_data(\n",
    "    preprocessed_test, \n",
    "    preprocessed_misconceptions, \n",
    "    test_predictions,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "print(f\"Prepared test reranking data with {len(test_reranking_data)} rows\")\n",
    "test_reranking_data.head()\n",
    "\n",
    "# Generate reranked predictions for test set\n",
    "reranked_test_predictions = generate_predictions_with_reranker(\n",
    "    trained_reranker,\n",
    "    test_reranking_data,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "print(f\"Generated reranked predictions for {len(reranked_test_predictions)} test questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions_for_submission(predictions):\n",
    "    \"\"\"\n",
    "    Format predictions for submission.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns QuestionId, MisconceptionIds\n",
    "    \"\"\"\n",
    "    submission_data = []\n",
    "    \n",
    "    for question_id, preds in predictions.items():\n",
    "        # Get misconception IDs\n",
    "        misconception_ids = [mid for mid, _ in preds]\n",
    "        \n",
    "        # Convert to string\n",
    "        misconception_ids_str = ' '.join(map(str, misconception_ids))\n",
    "        \n",
    "        # Add to submission data\n",
    "        submission_data.append({\n",
    "            'QuestionId': question_id,\n",
    "            'MisconceptionIds': misconception_ids_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(submission_data)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = format_predictions_for_submission(reranked_test_predictions)\n",
    "print(f\"Created submission with {len(submission_df)} rows\")\n",
    "submission_df.head()\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv('reranked_submission.csv', index=False)\n",
    "print(\"Submission saved to 'reranked_submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import time\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "preprocessed_train = train_data['preprocessed_train']\n",
    "preprocessed_test = train_data['preprocessed_test']\n",
    "preprocessed_misconceptions = train_data['preprocessed_misconceptions']\n",
    "training_pairs = train_data['training_pairs']\n",
    "cv_splits = train_data['cv_splits']\n",
    "misconception_to_idx = train_data['encodings']['misconception_to_idx']\n",
    "idx_to_misconception = train_data['encodings']['idx_to_misconception']\n",
    "\n",
    "print(f\"Loaded preprocessed train data with {len(preprocessed_train)} rows\")\n",
    "print(f\"Loaded preprocessed test data with {len(preprocessed_test)} rows\")\n",
    "print(f\"Loaded preprocessed misconceptions data with {len(preprocessed_misconceptions)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_unseen_misconception_folds(df, n_splits=5, unseen_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create cross-validation folds with stratified unseen misconceptions.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        n_splits: Number of folds\n",
    "        unseen_ratio: Proportion of misconceptions to hold out in each fold\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    # Get unique questions and misconceptions\n",
    "    unique_questions = df['QuestionId'].unique()\n",
    "    unique_misconceptions = df['MisconceptionId'].unique()\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Create folds\n",
    "    folds = []\n",
    "    \n",
    "    # Calculate misconceptions per fold for stratification\n",
    "    misc_per_fold = int(len(unique_misconceptions) * unseen_ratio)\n",
    "    \n",
    "    # Shuffle misconceptions to ensure random distribution across folds\n",
    "    shuffled_misconceptions = np.random.permutation(unique_misconceptions)\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        # Get misconceptions for this fold\n",
    "        start_idx = fold * misc_per_fold\n",
    "        end_idx = start_idx + misc_per_fold\n",
    "        if fold == n_splits - 1:  # Ensure all misconceptions are used\n",
    "            fold_misconceptions = shuffled_misconceptions[start_idx:]\n",
    "        else:\n",
    "            fold_misconceptions = shuffled_misconceptions[start_idx:end_idx]\n",
    "        \n",
    "        # Create validation indices (rows with misconceptions in this fold)\n",
    "        valid_idx = df[df['MisconceptionId'].isin(fold_misconceptions)].index\n",
    "        \n",
    "        # Create training indices (all other rows)\n",
    "        train_idx = df[~df['MisconceptionId'].isin(fold_misconceptions)].index\n",
    "        \n",
    "        folds.append((train_idx, valid_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_group_folds(df, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create cross-validation folds based on question groups.\n",
    "    This ensures that all rows related to the same question are in the same fold.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        n_splits: Number of folds\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get unique questions\n",
    "    unique_questions = df['QuestionId'].unique()\n",
    "    \n",
    "    # Shuffle questions to ensure random distribution across folds\n",
    "    shuffled_questions = np.random.permutation(unique_questions)\n",
    "    \n",
    "    # Create folds\n",
    "    folds = []\n",
    "    \n",
    "    # Calculate questions per fold\n",
    "    questions_per_fold = len(unique_questions) // n_splits\n",
    "    \n",
    "    for fold in range(n_splits):\n",
    "        # Get questions for this fold\n",
    "        start_idx = fold * questions_per_fold\n",
    "        end_idx = start_idx + questions_per_fold\n",
    "        if fold == n_splits - 1:  # Ensure all questions are used\n",
    "            fold_questions = shuffled_questions[start_idx:]\n",
    "        else:\n",
    "            fold_questions = shuffled_questions[start_idx:end_idx]\n",
    "        \n",
    "        # Create validation indices (rows with questions in this fold)\n",
    "        valid_idx = df[df['QuestionId'].isin(fold_questions)].index\n",
    "        \n",
    "        # Create training indices (all other rows)\n",
    "        train_idx = df[~df['QuestionId'].isin(fold_questions)].index\n",
    "        \n",
    "        folds.append((train_idx, valid_idx))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68688b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_folds(df, n_splits=5, unseen_ratio=0.2, question_ratio=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create hybrid cross-validation folds that simulate both unseen misconceptions and question groups.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        n_splits: Number of folds\n",
    "        unseen_ratio: Proportion of misconceptions to hold out in each fold\n",
    "        question_ratio: Proportion of validation data to come from question groups (vs. misconceptions)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    # Get misconception-based folds\n",
    "    misc_folds = create_stratified_unseen_misconception_folds(\n",
    "        df, n_splits=n_splits, unseen_ratio=unseen_ratio, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Get question-based folds\n",
    "    question_folds = create_question_group_folds(\n",
    "        df, n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create hybrid folds\n",
    "    hybrid_folds = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        # Get indices from both strategies\n",
    "        _, misc_valid_idx = misc_folds[i]\n",
    "        _, question_valid_idx = question_folds[i]\n",
    "        \n",
    "        # Combine validation indices\n",
    "        all_valid_idx = np.concatenate([misc_valid_idx, question_valid_idx])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        all_valid_idx = np.unique(all_valid_idx)\n",
    "        \n",
    "        # Create training indices (all other rows)\n",
    "        all_train_idx = np.array([idx for idx in df.index if idx not in all_valid_idx])\n",
    "        \n",
    "        hybrid_folds.append((all_train_idx, all_valid_idx))\n",
    "    \n",
    "    return hybrid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_retrieval_model_cv(df, misconceptions_df, folds, model_params, training_params):\n",
    "    \"\"\"\n",
    "    Train and evaluate a retrieval model using cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df: Preprocessed training dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        folds: List of (train_idx, valid_idx) pairs for cross-validation\n",
    "        model_params: Dictionary with model parameters (e.g., model name, embedding dim)\n",
    "        training_params: Dictionary with training parameters (e.g., batch size, learning rate)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation predictions for each fold and out-of-fold MAP@25 scores\n",
    "    \"\"\"\n",
    "    from section_3_functions import DualEncoder, train_dual_encoder, generate_predictions_with_dual_encoder\n",
    "    from section_3_functions import MisconceptionPairDataset, calculate_map_at_k\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'fold_predictions': {},\n",
    "        'fold_scores': {},\n",
    "        'out_of_fold_predictions': {}\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate on each fold\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"\\n{'-' * 40}\\nTraining on fold {fold_idx + 1}/{len(folds)}\\n{'-' * 40}\")\n",
    "        \n",
    "        # Get training and validation subsets\n",
    "        train_subset = df.iloc[train_idx]\n",
    "        valid_subset = df.iloc[valid_idx]\n",
    "        \n",
    "        # Create training pairs using only training data\n",
    "        # This function needs to be imported from section 3\n",
    "        train_pairs = create_training_pairs(train_subset, misconceptions_df)\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        model_name = model_params.get('model_name', 'sentence-transformers/all-mpnet-base-v2')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        train_dataset = MisconceptionPairDataset(train_pairs, tokenizer)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=training_params.get('batch_size', 8), \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DualEncoder(\n",
    "            model_name=model_name,\n",
    "            embedding_dim=model_params.get('embedding_dim', 768)\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, history = train_dual_encoder(\n",
    "            model,\n",
    "            train_loader,\n",
    "            epochs=training_params.get('epochs', 3),\n",
    "            lr=training_params.get('lr', 2e-5),\n",
    "            warmup_steps=training_params.get('warmup_steps', 0),\n",
    "            weight_decay=training_params.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Generate predictions for validation set\n",
    "        valid_predictions = generate_predictions_with_dual_encoder(\n",
    "            trained_model,\n",
    "            valid_subset,\n",
    "            misconceptions_df,\n",
    "            batch_size=training_params.get('eval_batch_size', 32),\n",
    "            top_k=25\n",
    "        )\n",
    "        \n",
    "        # Calculate MAP@25 on validation set\n",
    "        map_score = calculate_map_at_k(valid_predictions, valid_subset, k=25)\n",
    "        print(f\"Fold {fold_idx + 1} MAP@25: {map_score:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results['fold_predictions'][fold_idx] = valid_predictions\n",
    "        results['fold_scores'][fold_idx] = map_score\n",
    "        \n",
    "        # Add to out-of-fold predictions\n",
    "        for question_id, preds in valid_predictions.items():\n",
    "            results['out_of_fold_predictions'][question_id] = preds\n",
    "    \n",
    "    # Calculate overall MAP@25\n",
    "    overall_map = np.mean(list(results['fold_scores'].values()))\n",
    "    print(f\"\\nOverall MAP@25 across all folds: {overall_map:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92764c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reranker_cv(df, misconceptions_df, retrieval_predictions, folds, model_params, training_params):\n",
    "    \"\"\"\n",
    "    Train and evaluate a reranking model using cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df: Preprocessed training dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        retrieval_predictions: Dictionary with retrieval predictions for each fold\n",
    "        folds: List of (train_idx, valid_idx) pairs for cross-validation\n",
    "        model_params: Dictionary with model parameters (e.g., model name)\n",
    "        training_params: Dictionary with training parameters (e.g., batch size, learning rate)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with reranked predictions for each fold and out-of-fold MAP@25 scores\n",
    "    \"\"\"\n",
    "    from section_4_functions import RerankerDataset, train_reranker, generate_predictions_with_reranker\n",
    "    from section_4_functions import prepare_reranking_data\n",
    "    from section_3_functions import calculate_map_at_k\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'fold_predictions': {},\n",
    "        'fold_scores': {},\n",
    "        'out_of_fold_predictions': {}\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate on each fold\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"\\n{'-' * 40}\\nTraining reranker on fold {fold_idx + 1}/{len(folds)}\\n{'-' * 40}\")\n",
    "        \n",
    "        # Get training and validation subsets\n",
    "        train_subset = df.iloc[train_idx]\n",
    "        valid_subset = df.iloc[valid_idx]\n",
    "        \n",
    "        # Get retrieval predictions for this fold\n",
    "        fold_retrieval_predictions = retrieval_predictions['fold_predictions'][fold_idx]\n",
    "        \n",
    "        # Prepare reranking data\n",
    "        reranking_data = prepare_reranking_data(\n",
    "            valid_subset,\n",
    "            misconceptions_df,\n",
    "            fold_retrieval_predictions,\n",
    "            ground_truth=valid_subset,\n",
    "            top_k=25\n",
    "        )\n",
    "        \n",
    "        # Split reranking data into train and validation sets\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        reranker_train_df, reranker_val_df = train_test_split(\n",
    "            reranking_data, \n",
    "            test_size=0.2, \n",
    "            stratify=reranking_data['Label'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model_name = model_params.get('model_name', 'bert-base-uncased')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = RerankerDataset(reranker_train_df, tokenizer)\n",
    "        val_dataset = RerankerDataset(reranker_val_df, tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=training_params.get('batch_size', 8), \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=training_params.get('batch_size', 8), \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trained_reranker, history = train_reranker(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=training_params.get('epochs', 3),\n",
    "            lr=training_params.get('lr', 2e-5),\n",
    "            warmup_ratio=training_params.get('warmup_ratio', 0.1),\n",
    "            weight_decay=training_params.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Generate reranked predictions\n",
    "        reranked_predictions = generate_predictions_with_reranker(\n",
    "            trained_reranker,\n",
    "            reranking_data,\n",
    "            batch_size=training_params.get('eval_batch_size', 16),\n",
    "            top_k=25\n",
    "        )\n",
    "        \n",
    "        # Calculate MAP@25\n",
    "        reranker_map = calculate_map_at_k(reranked_predictions, valid_subset, k=25)\n",
    "        original_map = calculate_map_at_k(fold_retrieval_predictions, valid_subset, k=25)\n",
    "        \n",
    "        print(f\"Fold {fold_idx + 1} Original MAP@25: {original_map:.4f}\")\n",
    "        print(f\"Fold {fold_idx + 1} Reranked MAP@25: {reranker_map:.4f}\")\n",
    "        print(f\"Fold {fold_idx + 1} Improvement: {reranker_map - original_map:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results['fold_predictions'][fold_idx] = reranked_predictions\n",
    "        results['fold_scores'][fold_idx] = reranker_map\n",
    "        \n",
    "        # Add to out-of-fold predictions\n",
    "        for question_id, preds in reranked_predictions.items():\n",
    "            results['out_of_fold_predictions'][question_id] = preds\n",
    "    \n",
    "    # Calculate overall MAP@25\n",
    "    overall_map = np.mean(list(results['fold_scores'].values()))\n",
    "    print(f\"\\nOverall Reranked MAP@25 across all folds: {overall_map:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d783a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_average_ensemble(prediction_dicts, weights=None):\n",
    "    \"\"\"\n",
    "    Create an ensemble of predictions using simple averaging.\n",
    "    \n",
    "    Args:\n",
    "        prediction_dicts: List of dictionaries with predictions (QuestionId -> [(MisconceptionId, score)])\n",
    "        weights: Optional list of weights for each prediction dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ensembled predictions\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(prediction_dicts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = [w / sum(weights) for w in weights]\n",
    "    \n",
    "    # Get all question IDs\n",
    "    all_question_ids = set()\n",
    "    for pred_dict in prediction_dicts:\n",
    "        all_question_ids.update(pred_dict.keys())\n",
    "    \n",
    "    # Create ensembled predictions\n",
    "    ensembled_predictions = {}\n",
    "    \n",
    "    for question_id in all_question_ids:\n",
    "        # Collect all predictions for this question\n",
    "        all_preds = defaultdict(float)\n",
    "        \n",
    "        for i, pred_dict in enumerate(prediction_dicts):\n",
    "            if question_id in pred_dict:\n",
    "                for misconception_id, score in pred_dict[question_id]:\n",
    "                    all_preds[misconception_id] += score * weights[i]\n",
    "        \n",
    "        # Sort by score and keep top 25\n",
    "        sorted_preds = sorted(all_preds.items(), key=lambda x: x[1], reverse=True)[:25]\n",
    "        \n",
    "        # Store in ensembled predictions\n",
    "        ensembled_predictions[question_id] = sorted_preds\n",
    "    \n",
    "    return ensembled_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c22178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_based_ensemble(prediction_dicts, weights=None, top_k=25):\n",
    "    \"\"\"\n",
    "    Create an ensemble of predictions using rank-based weighting.\n",
    "    \n",
    "    Args:\n",
    "        prediction_dicts: List of dictionaries with predictions (QuestionId -> [(MisconceptionId, score)])\n",
    "        weights: Optional list of weights for each prediction dictionary\n",
    "        top_k: Number of top predictions to include\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ensembled predictions\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(prediction_dicts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = [w / sum(weights) for w in weights]\n",
    "    \n",
    "    # Get all question IDs\n",
    "    all_question_ids = set()\n",
    "    for pred_dict in prediction_dicts:\n",
    "        all_question_ids.update(pred_dict.keys())\n",
    "    \n",
    "    # Create ensembled predictions\n",
    "    ensembled_predictions = {}\n",
    "    \n",
    "    for question_id in all_question_ids:\n",
    "        # Collect all predictions for this question with rank-based scoring\n",
    "        all_preds = defaultdict(float)\n",
    "        \n",
    "        for i, pred_dict in enumerate(prediction_dicts):\n",
    "            if question_id in pred_dict:\n",
    "                for rank, (misconception_id, score) in enumerate(pred_dict[question_id][:top_k]):\n",
    "                    # Use rank-based weighting (inverse rank)\n",
    "                    rank_weight = 1.0 / (rank + 1)\n",
    "                    all_preds[misconception_id] += rank_weight * weights[i]\n",
    "        \n",
    "        # Sort by score and keep top 25\n",
    "        sorted_preds = sorted(all_preds.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        if sorted_preds:\n",
    "            max_score = sorted_preds[0][1]\n",
    "            normalized_preds = [(mid, score / max_score) for mid, score in sorted_preds]\n",
    "        else:\n",
    "            normalized_preds = []\n",
    "        \n",
    "        # Store in ensembled predictions\n",
    "        ensembled_predictions[question_id] = normalized_preds\n",
    "    \n",
    "    return ensembled_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ea96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hybrid folds\n",
    "n_splits = 5\n",
    "hybrid_folds = create_hybrid_folds(\n",
    "    preprocessed_train, \n",
    "    n_splits=n_splits, \n",
    "    unseen_ratio=0.2, \n",
    "    question_ratio=0.5\n",
    ")\n",
    "\n",
    "# Check the fold distribution\n",
    "for fold_idx, (train_idx, valid_idx) in enumerate(hybrid_folds):\n",
    "    train_subset = preprocessed_train.iloc[train_idx]\n",
    "    valid_subset = preprocessed_train.iloc[valid_idx]\n",
    "    \n",
    "    train_misconceptions = set(train_subset['MisconceptionId'])\n",
    "    valid_misconceptions = set(valid_subset['MisconceptionId'])\n",
    "    unseen_misconceptions = valid_misconceptions - train_misconceptions\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1}:\")\n",
    "    print(f\"  Train set size: {len(train_subset)}\")\n",
    "    print(f\"  Validation set size: {len(valid_subset)}\")\n",
    "    print(f\"  Number of unseen misconceptions in validation: {len(unseen_misconceptions)}\")\n",
    "    print(f\"  Proportion of unseen misconceptions: {len(unseen_misconceptions) / len(valid_misconceptions):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define missing classes and functions\n",
    "\n",
    "class DualEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual encoder model for learning query and misconception embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-mpnet-base-v2\", embedding_dim=768):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.query_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.misconception_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Projection layers\n",
    "        self.query_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.misconception_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to get sentence embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def encode_query(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Encode query inputs.\n",
    "        \"\"\"\n",
    "        outputs = self.query_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return self.query_proj(embeddings)\n",
    "    \n",
    "    def encode_misconception(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Encode misconception inputs.\n",
    "        \"\"\"\n",
    "        outputs = self.misconception_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return self.misconception_proj(embeddings)\n",
    "    \n",
    "    def forward(self, query_input_ids, query_attention_mask, misc_input_ids, misc_attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass to calculate similarity between query and misconception.\n",
    "        \"\"\"\n",
    "        query_embeddings = self.encode_query(query_input_ids, query_attention_mask)\n",
    "        misc_embeddings = self.encode_misconception(misc_input_ids, misc_attention_mask)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "        misc_embeddings = F.normalize(misc_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        return torch.matmul(query_embeddings, misc_embeddings.transpose(0, 1))\n",
    "\n",
    "class MisconceptionPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training with pairs of queries and misconceptions.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs_df, tokenizer, max_length=512):\n",
    "        self.pairs_df = pairs_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs_df.iloc[idx]\n",
    "        \n",
    "        # Get query text and positive misconception text\n",
    "        query_text = pair['ProcessedQuery']\n",
    "        pos_misc_text = pair['ProcessedPositiveMisconceptionText']\n",
    "        \n",
    "        # Randomly select one negative misconception\n",
    "        neg_misconceptions = pair['NegativeMisconceptions']\n",
    "        if len(neg_misconceptions) > 0:\n",
    "            neg_misc = random.choice(neg_misconceptions)\n",
    "            neg_misc_text = neg_misc['ProcessedMisconceptionText']\n",
    "        else:\n",
    "            # If no negative misconceptions available, use the positive one but with a label of 0\n",
    "            neg_misc_text = pos_misc_text\n",
    "        \n",
    "        # Tokenize texts\n",
    "        query_encoding = self.tokenizer(\n",
    "            query_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        pos_encoding = self.tokenizer(\n",
    "            pos_misc_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        neg_encoding = self.tokenizer(\n",
    "            neg_misc_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'query_input_ids': query_encoding['input_ids'].squeeze(),\n",
    "            'query_attention_mask': query_encoding['attention_mask'].squeeze(),\n",
    "            'pos_input_ids': pos_encoding['input_ids'].squeeze(),\n",
    "            'pos_attention_mask': pos_encoding['attention_mask'].squeeze(),\n",
    "            'neg_input_ids': neg_encoding['input_ids'].squeeze(),\n",
    "            'neg_attention_mask': neg_encoding['attention_mask'].squeeze(),\n",
    "            'question_id': pair['QuestionId'],\n",
    "            'pos_misconception_id': pair['PositiveMisconceptionId']\n",
    "        }\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss for training the dual encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, query_embeddings, pos_embeddings, neg_embeddings):\n",
    "        \"\"\"\n",
    "        Calculate triplet loss.\n",
    "        \n",
    "        Args:\n",
    "            query_embeddings: Embeddings of queries\n",
    "            pos_embeddings: Embeddings of positive misconceptions\n",
    "            neg_embeddings: Embeddings of negative misconceptions\n",
    "        \"\"\"\n",
    "        # Normalize embeddings\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "        pos_embeddings = F.normalize(pos_embeddings, p=2, dim=1)\n",
    "        neg_embeddings = F.normalize(neg_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        pos_similarity = torch.sum(query_embeddings * pos_embeddings, dim=1)\n",
    "        neg_similarity = torch.sum(query_embeddings * neg_embeddings, dim=1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = torch.clamp(self.margin - pos_similarity + neg_similarity, min=0)\n",
    "        return loss.mean()\n",
    "\n",
    "def train_dual_encoder(model, train_loader, val_loader=None, epochs=3, lr=2e-5, warmup_steps=0, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "    Train the dual encoder model.\n",
    "    \n",
    "    Args:\n",
    "        model: DualEncoder model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data (optional)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        warmup_steps: Number of warmup steps for learning rate scheduler\n",
    "        weight_decay: Weight decay for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set up loss function\n",
    "    triplet_loss = TripletLoss(margin=0.5)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            query_input_ids = batch['query_input_ids'].to(device)\n",
    "            query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "            pos_input_ids = batch['pos_input_ids'].to(device)\n",
    "            pos_attention_mask = batch['pos_attention_mask'].to(device)\n",
    "            neg_input_ids = batch['neg_input_ids'].to(device)\n",
    "            neg_attention_mask = batch['neg_attention_mask'].to(device)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            query_embeddings = model.encode_query(query_input_ids, query_attention_mask)\n",
    "            pos_embeddings = model.encode_misconception(pos_input_ids, pos_attention_mask)\n",
    "            neg_embeddings = model.encode_misconception(neg_input_ids, neg_attention_mask)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = triplet_loss(query_embeddings, pos_embeddings, neg_embeddings)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    # Move batch to device\n",
    "                    query_input_ids = batch['query_input_ids'].to(device)\n",
    "                    query_attention_mask = batch['query_attention_mask'].to(device)\n",
    "                    pos_input_ids = batch['pos_input_ids'].to(device)\n",
    "                    pos_attention_mask = batch['pos_attention_mask'].to(device)\n",
    "                    neg_input_ids = batch['neg_input_ids'].to(device)\n",
    "                    neg_attention_mask = batch['neg_attention_mask'].to(device)\n",
    "                    \n",
    "                    # Generate embeddings\n",
    "                    query_embeddings = model.encode_query(query_input_ids, query_attention_mask)\n",
    "                    pos_embeddings = model.encode_misconception(pos_input_ids, pos_attention_mask)\n",
    "                    neg_embeddings = model.encode_misconception(neg_input_ids, neg_attention_mask)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = triplet_loss(query_embeddings, pos_embeddings, neg_embeddings)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def generate_predictions_with_dual_encoder(model, test_df, misconceptions_df, batch_size=32, top_k=25):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained dual encoder model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DualEncoder model\n",
    "        test_df: Preprocessed test dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        batch_size: Batch size for generating embeddings\n",
    "        top_k: Number of top misconceptions to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions for test queries\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.query_encoder.config._name_or_path)\n",
    "    \n",
    "    # Generate query embeddings\n",
    "    query_embeddings = []\n",
    "    for i in tqdm(range(0, len(test_df), batch_size), desc=\"Generating query embeddings\"):\n",
    "        batch_texts = test_df['ProcessedQuery'].iloc[i:i+batch_size].tolist()\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode_query(\n",
    "                encoded_input['input_ids'], \n",
    "                encoded_input['attention_mask']\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            query_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    query_embeddings = np.vstack(query_embeddings)\n",
    "    \n",
    "    # Generate misconception embeddings\n",
    "    misconception_embeddings = []\n",
    "    for i in tqdm(range(0, len(misconceptions_df), batch_size), desc=\"Generating misconception embeddings\"):\n",
    "        batch_texts = misconceptions_df['ProcessedMisconceptionText'].iloc[i:i+batch_size].tolist()\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode_misconception(\n",
    "                encoded_input['input_ids'], \n",
    "                encoded_input['attention_mask']\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            misconception_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    misconception_embeddings = np.vstack(misconception_embeddings)\n",
    "    \n",
    "    # Compute similarity between queries and misconceptions\n",
    "    similarity_matrix = np.matmul(query_embeddings, misconception_embeddings.T)\n",
    "    \n",
    "    # Get top-k misconceptions for each query\n",
    "    predictions = {}\n",
    "    for i, question_id in enumerate(test_df['QuestionId']):\n",
    "        # Get similarity scores for this query\n",
    "        scores = similarity_matrix[i]\n",
    "        \n",
    "        # Get indices of top-k misconceptions\n",
    "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Get misconception IDs and scores\n",
    "        top_misconceptions = [misconceptions_df.iloc[idx]['MisconceptionId'] for idx in top_indices]\n",
    "        top_scores = [float(scores[idx]) for idx in top_indices]\n",
    "        \n",
    "        # Store predictions\n",
    "        predictions[question_id] = list(zip(top_misconceptions, top_scores))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_training_pairs(train_df, misconceptions_df, n_negatives=5, seed=42):\n",
    "    \"\"\"\n",
    "    Create training pairs with positive and negative examples.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Preprocessed training dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        n_negatives: Number of negative examples per positive\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with query, positive misconception, and negative misconceptions\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get all unique misconception IDs\n",
    "    all_misconception_ids = misconceptions_df['MisconceptionId'].unique()\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    \n",
    "    # Group by QuestionId to keep track of positive misconceptions\n",
    "    for question_id, group in train_df.groupby('QuestionId'):\n",
    "        # Get the positive misconceptions for this question\n",
    "        positive_misconceptions = group['MisconceptionId'].tolist()\n",
    "        \n",
    "        # Get the query for this question (should be the same for all rows)\n",
    "        query = group['Query'].iloc[0]\n",
    "        processed_query = group['ProcessedQuery'].iloc[0]\n",
    "        \n",
    "        # For each positive misconception, sample negative misconceptions\n",
    "        for pos_mid in positive_misconceptions:\n",
    "            # Get the processed text for the positive misconception\n",
    "            pos_misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == pos_mid]['MisconceptionText'].iloc[0]\n",
    "            pos_processed_text = misconceptions_df[misconceptions_df['MisconceptionId'] == pos_mid]['ProcessedMisconceptionText'].iloc[0]\n",
    "            \n",
    "            # Sample negative misconceptions (those not positive for this question)\n",
    "            negative_misconception_ids = np.random.choice(\n",
    "                [mid for mid in all_misconception_ids if mid not in positive_misconceptions],\n",
    "                size=min(n_negatives, len(all_misconception_ids) - len(positive_misconceptions)),\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            # Get the text for the negative misconceptions\n",
    "            negative_misconceptions = []\n",
    "            for neg_mid in negative_misconception_ids:\n",
    "                neg_misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == neg_mid]['MisconceptionText'].iloc[0]\n",
    "                neg_processed_text = misconceptions_df[misconceptions_df['MisconceptionId'] == neg_mid]['ProcessedMisconceptionText'].iloc[0]\n",
    "                \n",
    "                negative_misconceptions.append({\n",
    "                    'MisconceptionId': neg_mid,\n",
    "                    'MisconceptionText': neg_misconception_text,\n",
    "                    'ProcessedMisconceptionText': neg_processed_text\n",
    "                })\n",
    "            \n",
    "            # Add the data point\n",
    "            data.append({\n",
    "                'QuestionId': question_id,\n",
    "                'Query': query,\n",
    "                'ProcessedQuery': processed_query,\n",
    "                'PositiveMisconceptionId': pos_mid,\n",
    "                'PositiveMisconceptionText': pos_misconception_text,\n",
    "                'ProcessedPositiveMisconceptionText': pos_processed_text,\n",
    "                'NegativeMisconceptions': negative_misconceptions\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def train_retrieval_model_cv(df, misconceptions_df, folds, model_params, training_params):\n",
    "    \"\"\"\n",
    "    Train and evaluate a retrieval model using cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df: Preprocessed training dataframe\n",
    "        misconceptions_df: Preprocessed misconceptions dataframe\n",
    "        folds: List of (train_idx, valid_idx) pairs for cross-validation\n",
    "        model_params: Dictionary with model parameters (e.g., model name, embedding dim)\n",
    "        training_params: Dictionary with training parameters (e.g., batch size, learning rate)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with validation predictions for each fold and out-of-fold MAP@25 scores\n",
    "    \"\"\"\n",
    "    # Results storage\n",
    "    results = {\n",
    "        'fold_predictions': {},\n",
    "        'fold_scores': {},\n",
    "        'out_of_fold_predictions': {}\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate on each fold\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"\\n{'-' * 40}\\nTraining on fold {fold_idx + 1}/{len(folds)}\\n{'-' * 40}\")\n",
    "        \n",
    "        # Get training and validation subsets\n",
    "        train_subset = df.iloc[train_idx]\n",
    "        valid_subset = df.iloc[valid_idx]\n",
    "        \n",
    "        # Create training pairs using only training data\n",
    "        train_pairs = create_training_pairs(train_subset, misconceptions_df)\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        model_name = model_params.get('model_name', 'sentence-transformers/all-mpnet-base-v2')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        train_dataset = MisconceptionPairDataset(train_pairs, tokenizer)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=training_params.get('batch_size', 8), \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = DualEncoder(\n",
    "            model_name=model_name,\n",
    "            embedding_dim=model_params.get('embedding_dim', 768)\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trained_model, history = train_dual_encoder(\n",
    "            model,\n",
    "            train_loader,\n",
    "            epochs=training_params.get('epochs', 3),\n",
    "            lr=training_params.get('lr', 2e-5),\n",
    "            warmup_steps=training_params.get('warmup_steps', 0),\n",
    "            weight_decay=training_params.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Generate predictions for validation set\n",
    "        valid_predictions = generate_predictions_with_dual_encoder(\n",
    "            trained_model,\n",
    "            valid_subset,\n",
    "            misconceptions_df,\n",
    "            batch_size=training_params.get('eval_batch_size', 32),\n",
    "            top_k=25\n",
    "        )\n",
    "        \n",
    "        # Calculate MAP@25 on validation set\n",
    "        map_score = calculate_map_at_k(valid_predictions, valid_subset, k=25)\n",
    "        print(f\"Fold {fold_idx + 1} MAP@25: {map_score:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results['fold_predictions'][fold_idx] = valid_predictions\n",
    "        results['fold_scores'][fold_idx] = map_score\n",
    "        \n",
    "        # Add to out-of-fold predictions\n",
    "        for question_id, preds in valid_predictions.items():\n",
    "            results['out_of_fold_predictions'][question_id] = preds\n",
    "    \n",
    "    # Calculate overall MAP@25\n",
    "    overall_map = np.mean(list(results['fold_scores'].values()))\n",
    "    print(f\"\\nOverall MAP@25 across all folds: {overall_map:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_folds(df, n_splits=5, unseen_ratio=0.2, question_ratio=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Create hybrid cross-validation folds that simulate both unseen misconceptions and question groups.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the training data\n",
    "        n_splits: Number of folds\n",
    "        unseen_ratio: Proportion of misconceptions to hold out in each fold\n",
    "        question_ratio: Proportion of validation data to come from question groups (vs. misconceptions)\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_idx, valid_idx) pairs for each fold\n",
    "    \"\"\"\n",
    "    # Create stratified unseen misconception folds\n",
    "    def create_stratified_unseen_misconception_folds(df, n_splits=5, unseen_ratio=0.2, random_state=42):\n",
    "        # Get unique misconceptions\n",
    "        unique_misconceptions = df['MisconceptionId'].unique()\n",
    "        \n",
    "        # Set random seed\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Create folds\n",
    "        folds = []\n",
    "        \n",
    "        # Calculate misconceptions per fold for stratification\n",
    "        misc_per_fold = int(len(unique_misconceptions) * unseen_ratio)\n",
    "        \n",
    "        # Shuffle misconceptions to ensure random distribution across folds\n",
    "        shuffled_misconceptions = np.random.permutation(unique_misconceptions)\n",
    "        \n",
    "        for fold in range(n_splits):\n",
    "            # Get misconceptions for this fold\n",
    "            start_idx = fold * misc_per_fold\n",
    "            end_idx = start_idx + misc_per_fold\n",
    "            if fold == n_splits - 1:  # Ensure all misconceptions are used\n",
    "                fold_misconceptions = shuffled_misconceptions[start_idx:]\n",
    "            else:\n",
    "                fold_misconceptions = shuffled_misconceptions[start_idx:end_idx]\n",
    "            \n",
    "            # Create validation indices (rows with misconceptions in this fold)\n",
    "            valid_idx = df[df['MisconceptionId'].isin(fold_misconceptions)].index\n",
    "            \n",
    "            # Create training indices (all other rows)\n",
    "            train_idx = df[~df['MisconceptionId'].isin(fold_misconceptions)].index\n",
    "            \n",
    "            folds.append((train_idx, valid_idx))\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    # Create question group folds\n",
    "    def create_question_group_folds(df, n_splits=5, random_state=42):\n",
    "        # Set random seed\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Get unique questions\n",
    "        unique_questions = df['QuestionId'].unique()\n",
    "        \n",
    "        # Shuffle questions to ensure random distribution across folds\n",
    "        shuffled_questions = np.random.permutation(unique_questions)\n",
    "        \n",
    "        # Create folds\n",
    "        folds = []\n",
    "        \n",
    "        # Calculate questions per fold\n",
    "        questions_per_fold = len(unique_questions) // n_splits\n",
    "        \n",
    "        for fold in range(n_splits):\n",
    "            # Get questions for this fold\n",
    "            start_idx = fold * questions_per_fold\n",
    "            end_idx = start_idx + questions_per_fold\n",
    "            if fold == n_splits - 1:  # Ensure all questions are used\n",
    "                fold_questions = shuffled_questions[start_idx:]\n",
    "            else:\n",
    "                fold_questions = shuffled_questions[start_idx:end_idx]\n",
    "            \n",
    "            # Create validation indices (rows with questions in this fold)\n",
    "            valid_idx = df[df['QuestionId'].isin(fold_questions)].index\n",
    "            \n",
    "            # Create training indices (all other rows)\n",
    "            train_idx = df[~df['QuestionId'].isin(fold_questions)].index\n",
    "            \n",
    "            folds.append((train_idx, valid_idx))\n",
    "        \n",
    "        return folds\n",
    "    \n",
    "    # Get misconception-based folds\n",
    "    misc_folds = create_stratified_unseen_misconception_folds(\n",
    "        df, n_splits=n_splits, unseen_ratio=unseen_ratio, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Get question-based folds\n",
    "    question_folds = create_question_group_folds(\n",
    "        df, n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create hybrid folds\n",
    "    hybrid_folds = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        # Get indices from both strategies\n",
    "        _, misc_valid_idx = misc_folds[i]\n",
    "        _, question_valid_idx = question_folds[i]\n",
    "        \n",
    "        # Combine validation indices\n",
    "        all_valid_idx = np.concatenate([misc_valid_idx, question_valid_idx])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        all_valid_idx = np.unique(all_valid_idx)\n",
    "        \n",
    "        # Create training indices (all other rows)\n",
    "        all_train_idx = np.array([idx for idx in df.index if idx not in all_valid_idx])\n",
    "        \n",
    "        hybrid_folds.append((all_train_idx, all_valid_idx))\n",
    "    \n",
    "    return hybrid_folds\n",
    "\n",
    "# Create hybrid folds\n",
    "n_splits = 5\n",
    "hybrid_folds = create_hybrid_folds(\n",
    "    preprocessed_train, \n",
    "    n_splits=n_splits, \n",
    "    unseen_ratio=0.2, \n",
    "    question_ratio=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ebb55",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define model configurations\n",
    "retrieval_model_configs = [\n",
    "    {\n",
    "        'name': 'mpnet',\n",
    "        'model_params': {\n",
    "            'model_name': 'sentence-transformers/all-mpnet-base-v2',\n",
    "            'embedding_dim': 768\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 8,\n",
    "            'epochs': 3,\n",
    "            'lr': 2e-5,\n",
    "            'eval_batch_size': 32\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'minilm',\n",
    "        'model_params': {\n",
    "            'model_name': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "            'embedding_dim': 384\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 16,\n",
    "            'epochs': 5,\n",
    "            'lr': 3e-5,\n",
    "            'eval_batch_size': 64\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'distilbert',\n",
    "        'model_params': {\n",
    "            'model_name': 'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
    "            'embedding_dim': 768\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 12,\n",
    "            'epochs': 4,\n",
    "            'lr': 2.5e-5,\n",
    "            'eval_batch_size': 48\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train retrieval models and store results\n",
    "retrieval_results = {}\n",
    "\n",
    "for config in retrieval_model_configs:\n",
    "    print(f\"\\n{'-' * 40}\\nTraining Retrieval Model: {config['name']}\\n{'-' * 40}\")\n",
    "    \n",
    "    # Train with cross-validation\n",
    "    results = train_retrieval_model_cv(\n",
    "        preprocessed_train,\n",
    "        preprocessed_misconceptions,\n",
    "        hybrid_folds,\n",
    "        config['model_params'],\n",
    "        config['training_params']\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    retrieval_results[config['name']] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reranker configurations\n",
    "reranker_model_configs = [\n",
    "    {\n",
    "        'name': 'bert',\n",
    "        'model_params': {\n",
    "            'model_name': 'bert-base-uncased'\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 8,\n",
    "            'epochs': 3,\n",
    "            'lr': 2e-5,\n",
    "            'eval_batch_size': 16\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'roberta',\n",
    "        'model_params': {\n",
    "            'model_name': 'roberta-base'\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 8,\n",
    "            'epochs': 3,\n",
    "            'lr': 1e-5,\n",
    "            'eval_batch_size': 16\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'distilbert',\n",
    "        'model_params': {\n",
    "            'model_name': 'distilbert-base-uncased'\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 12,\n",
    "            'epochs': 4,\n",
    "            'lr': 2.5e-5,\n",
    "            'eval_batch_size': 24\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train reranking models and store results\n",
    "reranker_results = {}\n",
    "\n",
    "for retrieval_name, retrieval_result in retrieval_results.items():\n",
    "    reranker_results[retrieval_name] = {}\n",
    "    \n",
    "    for config in reranker_model_configs:\n",
    "        print(f\"\\n{'-' * 40}\\nTraining Reranker: {config['name']} on {retrieval_name}\\n{'-' * 40}\")\n",
    "        \n",
    "        # Train with cross-validation\n",
    "        results = train_reranker_cv(\n",
    "            preprocessed_train,\n",
    "            preprocessed_misconceptions,\n",
    "            retrieval_result,\n",
    "            hybrid_folds,\n",
    "            config['model_params'],\n",
    "            config['training_params']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        reranker_results[retrieval_name][config['name']] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define the ensemble functions\n",
    "def simple_average_ensemble(prediction_dicts, weights=None):\n",
    "    \"\"\"\n",
    "    Create an ensemble of predictions using simple averaging.\n",
    "    \n",
    "    Args:\n",
    "        prediction_dicts: List of dictionaries with predictions (QuestionId -> [(MisconceptionId, score)])\n",
    "        weights: Optional list of weights for each prediction dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ensembled predictions\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(prediction_dicts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = [w / sum(weights) for w in weights]\n",
    "    \n",
    "    # Get all question IDs\n",
    "    all_question_ids = set()\n",
    "    for pred_dict in prediction_dicts:\n",
    "        all_question_ids.update(pred_dict.keys())\n",
    "    \n",
    "    # Create ensembled predictions\n",
    "    ensembled_predictions = {}\n",
    "    \n",
    "    for question_id in all_question_ids:\n",
    "        # Collect all predictions for this question\n",
    "        all_preds = defaultdict(float)\n",
    "        \n",
    "        for i, pred_dict in enumerate(prediction_dicts):\n",
    "            if question_id in pred_dict:\n",
    "                for misconception_id, score in pred_dict[question_id]:\n",
    "                    all_preds[misconception_id] += score * weights[i]\n",
    "        \n",
    "        # Sort by score and keep top 25\n",
    "        sorted_preds = sorted(all_preds.items(), key=lambda x: x[1], reverse=True)[:25]\n",
    "        \n",
    "        # Store in ensembled predictions\n",
    "        ensembled_predictions[question_id] = sorted_preds\n",
    "    \n",
    "    return ensembled_predictions\n",
    "\n",
    "def rank_based_ensemble(prediction_dicts, weights=None, top_k=25):\n",
    "    \"\"\"\n",
    "    Create an ensemble of predictions using rank-based weighting.\n",
    "    \n",
    "    Args:\n",
    "        prediction_dicts: List of dictionaries with predictions (QuestionId -> [(MisconceptionId, score)])\n",
    "        weights: Optional list of weights for each prediction dictionary\n",
    "        top_k: Number of top predictions to include\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ensembled predictions\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(prediction_dicts)\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = [w / sum(weights) for w in weights]\n",
    "    \n",
    "    # Get all question IDs\n",
    "    all_question_ids = set()\n",
    "    for pred_dict in prediction_dicts:\n",
    "        all_question_ids.update(pred_dict.keys())\n",
    "    \n",
    "    # Create ensembled predictions\n",
    "    ensembled_predictions = {}\n",
    "    \n",
    "    for question_id in all_question_ids:\n",
    "        # Collect all predictions for this question with rank-based scoring\n",
    "        all_preds = defaultdict(float)\n",
    "        \n",
    "        for i, pred_dict in enumerate(prediction_dicts):\n",
    "            if question_id in pred_dict:\n",
    "                for rank, (misconception_id, score) in enumerate(pred_dict[question_id][:top_k]):\n",
    "                    # Use rank-based weighting (inverse rank)\n",
    "                    rank_weight = 1.0 / (rank + 1)\n",
    "                    all_preds[misconception_id] += rank_weight * weights[i]\n",
    "        \n",
    "        # Sort by score and keep top 25\n",
    "        sorted_preds = sorted(all_preds.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        if sorted_preds:\n",
    "            max_score = sorted_preds[0][1]\n",
    "            normalized_preds = [(mid, score / max_score) for mid, score in sorted_preds]\n",
    "        else:\n",
    "            normalized_preds = []\n",
    "        \n",
    "        # Store in ensembled predictions\n",
    "        ensembled_predictions[question_id] = normalized_preds\n",
    "    \n",
    "    return ensembled_predictions\n",
    "\n",
    "# Then use this instead of importing from section_3_functions\n",
    "def calculate_map_at_k(predictions, ground_truth, k=25):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at k.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        k: k for MAP@k\n",
    "        \n",
    "    Returns:\n",
    "        MAP@k score\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Calculate AP@k for each question\n",
    "    ap_scores = []\n",
    "    for question_id, true_misconceptions in gt_by_question.items():\n",
    "        if question_id not in predictions:\n",
    "            ap_scores.append(0)\n",
    "            continue\n",
    "        \n",
    "        # Get predicted misconceptions\n",
    "        pred_misconceptions = [mid for mid, _ in predictions[question_id][:k]]\n",
    "        \n",
    "        # Calculate precision at each position\n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        \n",
    "        for i, mid in enumerate(pred_misconceptions):\n",
    "            if mid in true_misconceptions:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "        \n",
    "        # Calculate AP@k\n",
    "        if len(precisions) > 0:\n",
    "            ap = sum(precisions) / min(len(true_misconceptions), k)\n",
    "        else:\n",
    "            ap = 0\n",
    "        \n",
    "        ap_scores.append(ap)\n",
    "    \n",
    "    # Calculate MAP@k\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "# Now the ensemble code should work\n",
    "# Create a dictionary to hold all out-of-fold predictions\n",
    "all_oofs = {}\n",
    "\n",
    "# Add retrieval models\n",
    "for model_name, results in retrieval_results.items():\n",
    "    all_oofs[f\"retrieval_{model_name}\"] = results['out_of_fold_predictions']\n",
    "\n",
    "# Add reranker models\n",
    "for retrieval_name, rerankers in reranker_results.items():\n",
    "    for reranker_name, results in rerankers.items():\n",
    "        all_oofs[f\"reranked_{retrieval_name}_{reranker_name}\"] = results['out_of_fold_predictions']\n",
    "\n",
    "# Evaluate individual models on the full validation set\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "for model_name, predictions in all_oofs.items():\n",
    "    map_score = calculate_map_at_k(predictions, preprocessed_train, k=25)\n",
    "    print(f\"{model_name}: MAP@25 = {map_score:.4f}\")\n",
    "\n",
    "# Create and evaluate simple average ensemble\n",
    "simple_ensemble = simple_average_ensemble(list(all_oofs.values()))\n",
    "simple_ensemble_map = calculate_map_at_k(simple_ensemble, preprocessed_train, k=25)\n",
    "print(f\"\\nSimple Average Ensemble: MAP@25 = {simple_ensemble_map:.4f}\")\n",
    "\n",
    "# Create and evaluate rank-based ensemble\n",
    "rank_ensemble = rank_based_ensemble(list(all_oofs.values()))\n",
    "rank_ensemble_map = calculate_map_at_k(rank_ensemble, preprocessed_train, k=25)\n",
    "print(f\"Rank-Based Ensemble: MAP@25 = {rank_ensemble_map:.4f}\")\n",
    "\n",
    "# Try weighted ensembles based on individual performance\n",
    "model_performance = {}\n",
    "for model_name, predictions in all_oofs.items():\n",
    "    map_score = calculate_map_at_k(predictions, preprocessed_train, k=25)\n",
    "    model_performance[model_name] = map_score\n",
    "\n",
    "# Weights based on MAP scores\n",
    "map_weights = [model_performance[model_name] for model_name in all_oofs.keys()]\n",
    "weighted_ensemble = simple_average_ensemble(list(all_oofs.values()), weights=map_weights)\n",
    "weighted_ensemble_map = calculate_map_at_k(weighted_ensemble, preprocessed_train, k=25)\n",
    "print(f\"Weighted Ensemble: MAP@25 = {weighted_ensemble_map:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reranker model configurations first\n",
    "reranker_model_configs = [\n",
    "    {\n",
    "        'name': 'bert',\n",
    "        'model_params': {\n",
    "            'model_name': 'bert-base-uncased'\n",
    "        },\n",
    "        'training_params': {\n",
    "            'batch_size': 8,\n",
    "            'epochs': 3,\n",
    "            'lr': 2e-5,\n",
    "            'eval_batch_size': 16\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train final models on the full training data\n",
    "best_retrieval_config = retrieval_model_configs[0]  # MPNet\n",
    "best_reranker_config = reranker_model_configs[0]  # BERT\n",
    "\n",
    "print(\"\\nTraining final models on full training data...\")\n",
    "\n",
    "# Create training pairs\n",
    "train_pairs = create_training_pairs(preprocessed_train, preprocessed_misconceptions)\n",
    "\n",
    "# Initialize model\n",
    "model_name = best_retrieval_config['model_params']['model_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DualEncoder(\n",
    "    model_name=model_name,\n",
    "    embedding_dim=best_retrieval_config['model_params']['embedding_dim']\n",
    ")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = MisconceptionPairDataset(train_pairs, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=best_retrieval_config['training_params']['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_retrieval_model, _ = train_dual_encoder(\n",
    "    model,\n",
    "    train_loader,\n",
    "    epochs=best_retrieval_config['training_params']['epochs'],\n",
    "    lr=best_retrieval_config['training_params']['lr']\n",
    ")\n",
    "\n",
    "# Generate predictions for test set\n",
    "test_retrieval_predictions = generate_predictions_with_dual_encoder(\n",
    "    final_retrieval_model,\n",
    "    preprocessed_test,\n",
    "    preprocessed_misconceptions,\n",
    "    batch_size=best_retrieval_config['training_params']['eval_batch_size'],\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Define the missing RerankerDataset class\n",
    "class RerankerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for training a reranking model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Get query and misconception text\n",
    "        query_text = row['Query']\n",
    "        misconception_text = row['MisconceptionText']\n",
    "        \n",
    "        # Tokenize as a pair\n",
    "        encoding = self.tokenizer(\n",
    "            query_text,\n",
    "            misconception_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation='longest_first',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'token_type_ids': encoding.get('token_type_ids', torch.zeros_like(encoding['attention_mask'])).squeeze(),\n",
    "            'question_id': row['QuestionId'],\n",
    "            'misconception_id': row['MisconceptionId'],\n",
    "            'retrieval_score': torch.tensor(row['RetrievalScore'], dtype=torch.float32)\n",
    "        }\n",
    "        \n",
    "        # Add label for training data\n",
    "        if 'Label' in row:\n",
    "            item['label'] = torch.tensor(row['Label'], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "# Define the train_reranker function\n",
    "def train_reranker(model, train_loader, val_loader=None, epochs=3, lr=2e-5, warmup_ratio=0.1, weight_decay=0.01):\n",
    "    \"\"\"\n",
    "    Train a cross-encoder reranking model.\n",
    "    \"\"\"\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Set up learning rate scheduler\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=warmup_steps, \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Set up loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch.get('token_type_ids', torch.zeros_like(attention_mask)).to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += len(labels)\n",
    "            \n",
    "            # Update progress bar\n",
    "            acc = train_correct / train_total\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'acc': acc})\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    # Move batch to device\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    token_type_ids = batch.get('token_type_ids', torch.zeros_like(attention_mask)).to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Calculate accuracy\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    val_correct += (preds == labels).sum().item()\n",
    "                    val_total += len(labels)\n",
    "            \n",
    "            # Calculate average validation metrics\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_acc = val_correct / val_total\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Train Acc: {train_acc:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Train Acc: {train_acc:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Define the generate_predictions_with_reranker function\n",
    "def generate_predictions_with_reranker(model, test_df, batch_size=16, top_k=25):\n",
    "    \"\"\"\n",
    "    Generate predictions using the trained reranking model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.config._name_or_path)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    test_dataset = RerankerDataset(test_df, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Generating reranking predictions\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch.get('token_type_ids', torch.zeros_like(attention_mask)).to(device)\n",
    "            question_ids = batch['question_id']\n",
    "            misconception_ids = batch['misconception_id']\n",
    "            retrieval_scores = batch['retrieval_score']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            # Get logits and convert to probabilities\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            positive_probs = probs[:, 1].cpu().numpy()  # Probability of positive class\n",
    "            \n",
    "            # Calculate combined scores (weighted combination of retrieval and reranking scores)\n",
    "            alpha = 0.7  # Weight for reranking scores\n",
    "            combined_scores = alpha * positive_probs + (1 - alpha) * retrieval_scores.cpu().numpy()\n",
    "            \n",
    "            # Store predictions\n",
    "            for i in range(len(question_ids)):\n",
    "                question_id = question_ids[i].item()\n",
    "                misconception_id = misconception_ids[i].item()\n",
    "                score = combined_scores[i]\n",
    "                \n",
    "                if question_id not in predictions:\n",
    "                    predictions[question_id] = []\n",
    "                \n",
    "                predictions[question_id].append((misconception_id, score))\n",
    "    \n",
    "    # Sort predictions and keep top-k for each question\n",
    "    for question_id in predictions:\n",
    "        predictions[question_id] = sorted(predictions[question_id], key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Define the prepare_reranking_data function\n",
    "def prepare_reranking_data(query_df, misconceptions_df, retrieval_predictions, ground_truth=None, top_k=25, n_negatives=5):\n",
    "    \"\"\"\n",
    "    Prepare data for training/evaluating a reranking model.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    # Create a lookup for ground truth misconceptions\n",
    "    gt_by_question = {}\n",
    "    if ground_truth is not None:\n",
    "        for _, row in ground_truth.iterrows():\n",
    "            question_id = row['QuestionId']\n",
    "            misconception_id = row['MisconceptionId']\n",
    "            \n",
    "            if question_id not in gt_by_question:\n",
    "                gt_by_question[question_id] = []\n",
    "            \n",
    "            gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Process each query\n",
    "    for question_id, query_row in query_df.iterrows():\n",
    "        question_id = query_row['QuestionId']\n",
    "        \n",
    "        # Skip if no retrieval predictions for this question\n",
    "        if question_id not in retrieval_predictions:\n",
    "            continue\n",
    "        \n",
    "        # Get query text\n",
    "        query_text = query_row['Query']\n",
    "        processed_query = query_row['ProcessedQuery']\n",
    "        \n",
    "        # Get top-k misconceptions from retrieval predictions\n",
    "        top_misconceptions = retrieval_predictions[question_id][:top_k]\n",
    "        \n",
    "        if ground_truth is not None:\n",
    "            # This is for training data - create positive and negative examples\n",
    "            if question_id in gt_by_question:\n",
    "                true_misconceptions = gt_by_question[question_id]\n",
    "                \n",
    "                # Add all retrieved misconceptions as examples\n",
    "                for misconception_id, score in top_misconceptions:\n",
    "                    # Get misconception text\n",
    "                    misconception_row = misconceptions_df[misconceptions_df['MisconceptionId'] == misconception_id].iloc[0]\n",
    "                    misconception_text = misconception_row['MisconceptionText']\n",
    "                    processed_misconception = misconception_row['ProcessedMisconceptionText']\n",
    "                    \n",
    "                    # Determine label (1 for positive, 0 for negative)\n",
    "                    label = 1 if misconception_id in true_misconceptions else 0\n",
    "                    \n",
    "                    # Add to data\n",
    "                    data.append({\n",
    "                        'QuestionId': question_id,\n",
    "                        'MisconceptionId': misconception_id,\n",
    "                        'Query': query_text,\n",
    "                        'ProcessedQuery': processed_query,\n",
    "                        'MisconceptionText': misconception_text,\n",
    "                        'ProcessedMisconceptionText': processed_misconception,\n",
    "                        'RetrievalScore': score,\n",
    "                        'Label': label\n",
    "                    })\n",
    "                \n",
    "                # If we don't have enough positive examples, add some from ground truth\n",
    "                retrieved_positives = [mid for mid, _ in top_misconceptions if mid in true_misconceptions]\n",
    "                missing_positives = [mid for mid in true_misconceptions if mid not in retrieved_positives]\n",
    "                \n",
    "                for misconception_id in missing_positives:\n",
    "                    # Get misconception text\n",
    "                    misconception_row = misconceptions_df[misconceptions_df['MisconceptionId'] == misconception_id].iloc[0]\n",
    "                    misconception_text = misconception_row['MisconceptionText']\n",
    "                    processed_misconception = misconception_row['ProcessedMisconceptionText']\n",
    "                    \n",
    "                    # Add to data with a default retrieval score\n",
    "                    data.append({\n",
    "                        'QuestionId': question_id,\n",
    "                        'MisconceptionId': misconception_id,\n",
    "                        'Query': query_text,\n",
    "                        'ProcessedQuery': processed_query,\n",
    "                        'MisconceptionText': misconception_text,\n",
    "                        'ProcessedMisconceptionText': processed_misconception,\n",
    "                        'RetrievalScore': 0.0,  # Default score for missing examples\n",
    "                        'Label': 1\n",
    "                    })\n",
    "        else:\n",
    "            # This is for test data - just add all retrieved misconceptions\n",
    "            for misconception_id, score in top_misconceptions:\n",
    "                # Get misconception text\n",
    "                misconception_row = misconceptions_df[misconceptions_df['MisconceptionId'] == misconception_id].iloc[0]\n",
    "                misconception_text = misconception_row['MisconceptionText']\n",
    "                processed_misconception = misconception_row['ProcessedMisconceptionText']\n",
    "                \n",
    "                # Add to data (no label for test data)\n",
    "                data.append({\n",
    "                    'QuestionId': question_id,\n",
    "                    'MisconceptionId': misconception_id,\n",
    "                    'Query': query_text,\n",
    "                    'ProcessedQuery': processed_query,\n",
    "                    'MisconceptionText': misconception_text,\n",
    "                    'ProcessedMisconceptionText': processed_misconception,\n",
    "                    'RetrievalScore': score\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Now continue with the training of the reranking model\n",
    "# Prepare reranking data\n",
    "reranking_data = prepare_reranking_data(\n",
    "    preprocessed_train,\n",
    "    preprocessed_misconceptions,\n",
    "    test_retrieval_predictions,\n",
    "    ground_truth=preprocessed_train,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Split data for training\n",
    "reranker_train_df, reranker_val_df = train_test_split(\n",
    "    reranking_data, \n",
    "    test_size=0.2, \n",
    "    stratify=reranking_data['Label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "reranker_model_name = best_reranker_config['model_params']['model_name']\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name, num_labels=2)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "reranker_train_dataset = RerankerDataset(reranker_train_df, reranker_tokenizer)\n",
    "reranker_val_dataset = RerankerDataset(reranker_val_df, reranker_tokenizer)\n",
    "\n",
    "reranker_train_loader = DataLoader(\n",
    "    reranker_train_dataset, \n",
    "    batch_size=best_reranker_config['training_params']['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "reranker_val_loader = DataLoader(\n",
    "    reranker_val_dataset, \n",
    "    batch_size=best_reranker_config['training_params']['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "final_reranker_model, _ = train_reranker(\n",
    "    reranker_model,\n",
    "    reranker_train_loader,\n",
    "    val_loader=reranker_val_loader,\n",
    "    epochs=best_reranker_config['training_params']['epochs'],\n",
    "    lr=best_reranker_config['training_params']['lr']\n",
    ")\n",
    "\n",
    "# Prepare test data for reranking\n",
    "test_reranking_data = prepare_reranking_data(\n",
    "    preprocessed_test,\n",
    "    preprocessed_misconceptions,\n",
    "    test_retrieval_predictions,\n",
    "    top_k=25\n",
    ")\n",
    "\n",
    "# Generate reranked predictions\n",
    "test_reranked_predictions = generate_predictions_with_reranker(\n",
    "    final_reranker_model,\n",
    "    test_reranking_data,\n",
    "    batch_size=best_reranker_config['training_params']['eval_batch_size'],\n",
    "    top_k=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916585b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble of our two predictions\n",
    "ensemble_predictions = simple_average_ensemble(\n",
    "    [test_retrieval_predictions, test_reranked_predictions],\n",
    "    weights=[0.3, 0.7]  # Give more weight to the reranker\n",
    ")\n",
    "\n",
    "# Format predictions for submission\n",
    "def format_predictions_for_submission(predictions):\n",
    "    \"\"\"\n",
    "    Format predictions for submission.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns QuestionId, MisconceptionIds\n",
    "    \"\"\"\n",
    "    submission_data = []\n",
    "    \n",
    "    for question_id, preds in predictions.items():\n",
    "        # Get misconception IDs\n",
    "        misconception_ids = [mid for mid, _ in preds]\n",
    "        \n",
    "        # Convert to string\n",
    "        misconception_ids_str = ' '.join(map(str, misconception_ids))\n",
    "        \n",
    "        # Add to submission data\n",
    "        submission_data.append({\n",
    "            'QuestionId': question_id,\n",
    "            'MisconceptionIds': misconception_ids_str\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(submission_data)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = format_predictions_for_submission(ensemble_predictions)\n",
    "print(f\"Created submission with {len(submission_df)} rows\")\n",
    "submission_df.head()\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv('final_ensemble_submission.csv', index=False)\n",
    "print(\"Submission saved to 'final_ensemble_submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40176e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "with open('preprocessed_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "preprocessed_train = train_data['preprocessed_train']\n",
    "preprocessed_test = train_data['preprocessed_test']\n",
    "preprocessed_misconceptions = train_data['preprocessed_misconceptions']\n",
    "\n",
    "# Load predictions from previous sections\n",
    "with open('ensemble_predictions.pkl', 'rb') as f:\n",
    "    ensemble_predictions = pickle.load(f)\n",
    "\n",
    "final_predictions = ensemble_predictions['final_predictions']\n",
    "\n",
    "print(f\"Loaded preprocessed train data with {len(preprocessed_train)} rows\")\n",
    "print(f\"Loaded preprocessed test data with {len(preprocessed_test)} rows\")\n",
    "print(f\"Loaded final predictions for {len(final_predictions)} test questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for model optimization\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "\n",
    "def optimize_model_for_inference(model, model_type=\"retrieval\"):\n",
    "    \"\"\"\n",
    "    Optimize a model for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model (PyTorch)\n",
    "        model_type: Type of model (\"retrieval\" or \"reranking\")\n",
    "        \n",
    "    Returns:\n",
    "        Optimized model and tokenizer\n",
    "    \"\"\"\n",
    "    # Convert to ONNX for faster inference\n",
    "    if model_type == \"retrieval\":\n",
    "        # For dual encoder models\n",
    "        dummy_input_ids = torch.ones(1, 128, dtype=torch.long).to(device)\n",
    "        dummy_attention_mask = torch.ones(1, 128, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Export query encoder\n",
    "        torch.onnx.export(\n",
    "            model.query_encoder,\n",
    "            (dummy_input_ids, dummy_attention_mask),\n",
    "            \"query_encoder.onnx\",\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],\n",
    "            output_names=[\"last_hidden_state\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"last_hidden_state\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Export misconception encoder\n",
    "        torch.onnx.export(\n",
    "            model.misconception_encoder,\n",
    "            (dummy_input_ids, dummy_attention_mask),\n",
    "            \"misconception_encoder.onnx\",\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],\n",
    "            output_names=[\"last_hidden_state\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"last_hidden_state\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create ONNX Runtime sessions\n",
    "        query_session = ort.InferenceSession(\"query_encoder.onnx\")\n",
    "        misconception_session = ort.InferenceSession(\"misconception_encoder.onnx\")\n",
    "        \n",
    "        # Return sessions\n",
    "        return (query_session, misconception_session)\n",
    "    \n",
    "    else:  # reranking\n",
    "        # For cross-encoder models\n",
    "        dummy_input_ids = torch.ones(1, 256, dtype=torch.long).to(device)\n",
    "        dummy_attention_mask = torch.ones(1, 256, dtype=torch.long).to(device)\n",
    "        dummy_token_type_ids = torch.zeros(1, 256, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Export model\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (dummy_input_ids, dummy_attention_mask, dummy_token_type_ids),\n",
    "            \"reranker.onnx\",\n",
    "            input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "                \"logits\": {0: \"batch_size\"}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create ONNX Runtime session\n",
    "        session = ort.InferenceSession(\"reranker.onnx\")\n",
    "        \n",
    "        # Return session\n",
    "        return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ba016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "# Define the DualEncoder class\n",
    "class DualEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual encoder model for learning query and misconception embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\", embedding_dim=384):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.query_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.misconception_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Projection layers\n",
    "        self.query_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.misconception_proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Mean pooling to get sentence embeddings.\n",
    "        \"\"\"\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def encode_query(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Encode query inputs.\n",
    "        \"\"\"\n",
    "        outputs = self.query_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return self.query_proj(embeddings)\n",
    "    \n",
    "    def encode_misconception(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Encode misconception inputs.\n",
    "        \"\"\"\n",
    "        outputs = self.misconception_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = self.mean_pooling(outputs, attention_mask)\n",
    "        return self.misconception_proj(embeddings)\n",
    "    \n",
    "    def forward(self, query_input_ids, query_attention_mask, misc_input_ids, misc_attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass to calculate similarity between query and misconception.\n",
    "        \"\"\"\n",
    "        query_embeddings = self.encode_query(query_input_ids, query_attention_mask)\n",
    "        misc_embeddings = self.encode_misconception(misc_input_ids, misc_attention_mask)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        query_embeddings = F.normalize(query_embeddings, p=2, dim=1)\n",
    "        misc_embeddings = F.normalize(misc_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        return torch.matmul(query_embeddings, misc_embeddings.transpose(0, 1))\n",
    "\n",
    "# Get model size function\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize smaller retrieval model\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"  # Smaller model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "retrieval_model = DualEncoder(model_name=model_name, embedding_dim=384)\n",
    "\n",
    "print(f\"Retrieval model size: {get_model_size(retrieval_model):.2f} MB\")\n",
    "\n",
    "# Create lightweight reranking model\n",
    "reranker_model_name = \"distilbert-base-uncased\"  # Smaller model\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name, num_labels=2)\n",
    "\n",
    "print(f\"Reranker model size: {get_model_size(reranker_model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3816944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference_speed(model, model_type=\"retrieval\", num_iterations=100):\n",
    "    \"\"\"\n",
    "    Benchmark the inference speed of a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to benchmark\n",
    "        model_type: Type of model (\"retrieval\" or \"reranking\")\n",
    "        num_iterations: Number of iterations for benchmarking\n",
    "        \n",
    "    Returns:\n",
    "        Average inference time (in ms)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if model_type == \"retrieval\":\n",
    "        # Create dummy inputs for retrieval model\n",
    "        input_ids = torch.ones(1, 128, dtype=torch.long).to(device)\n",
    "        attention_mask = torch.ones(1, 128, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = model.encode_query(input_ids, attention_mask)\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            with torch.no_grad():\n",
    "                _ = model.encode_query(input_ids, attention_mask)\n",
    "        end_time = time.time()\n",
    "    \n",
    "    else:  # reranking\n",
    "        # Create dummy inputs for reranking model\n",
    "        input_ids = torch.ones(1, 256, dtype=torch.long).to(device)\n",
    "        attention_mask = torch.ones(1, 256, dtype=torch.long).to(device)\n",
    "        token_type_ids = torch.zeros(1, 256, dtype=torch.long).to(device)\n",
    "        \n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_iterations):\n",
    "            with torch.no_grad():\n",
    "                _ = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        end_time = time.time()\n",
    "    \n",
    "    # Calculate average inference time\n",
    "    avg_time_ms = (end_time - start_time) * 1000 / num_iterations\n",
    "    \n",
    "    return avg_time_ms\n",
    "\n",
    "# Benchmark retrieval model\n",
    "retrieval_time = benchmark_inference_speed(retrieval_model, model_type=\"retrieval\")\n",
    "print(f\"Retrieval model average inference time: {retrieval_time:.2f} ms\")\n",
    "\n",
    "# Benchmark reranking model\n",
    "reranker_time = benchmark_inference_speed(reranker_model, model_type=\"reranking\")\n",
    "print(f\"Reranker model average inference time: {reranker_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for quantization\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "import torch.quantization\n",
    "\n",
    "# Quantize the reranker model\n",
    "def quantize_reranker_model(model):\n",
    "    \"\"\"\n",
    "    Quantize a reranker model for efficient inference.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained reranker model\n",
    "        \n",
    "    Returns:\n",
    "        Quantized model\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to static quantized model\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},  # Specify which layers to quantize\n",
    "        dtype=torch.qint8  # 8-bit quantization\n",
    "    )\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "# Quantize reranker model\n",
    "quantized_reranker = quantize_reranker_model(reranker_model)\n",
    "\n",
    "# Get model size (in MB)\n",
    "print(f\"Original reranker model size: {get_model_size(reranker_model):.2f} MB\")\n",
    "print(f\"Quantized reranker model size: {get_model_size(quantized_reranker):.2f} MB\")\n",
    "\n",
    "# Benchmark quantized model\n",
    "quantized_reranker_time = benchmark_inference_speed(quantized_reranker, model_type=\"reranking\")\n",
    "print(f\"Quantized reranker model average inference time: {quantized_reranker_time:.2f} ms\")\n",
    "print(f\"Speed improvement: {reranker_time / quantized_reranker_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db9463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_inference_pipeline(query_texts, misconception_texts, tokenizer, retrieval_model, reranker_model, top_k=25):\n",
    "    \"\"\"\n",
    "    Efficient inference pipeline for the efficiency track.\n",
    "    \n",
    "    Args:\n",
    "        query_texts: List of query texts\n",
    "        misconception_texts: List of misconception texts\n",
    "        tokenizer: Tokenizer for both models\n",
    "        retrieval_model: Retrieval model\n",
    "        reranker_model: Reranker model\n",
    "        top_k: Number of top misconceptions to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions\n",
    "    \"\"\"\n",
    "    # Set models to evaluation mode\n",
    "    retrieval_model.eval()\n",
    "    reranker_model.eval()\n",
    "    \n",
    "    # Step 1: Generate embeddings for all misconceptions (once)\n",
    "    misconception_embeddings = []\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(misconception_texts), batch_size), desc=\"Generating misconception embeddings\"):\n",
    "        batch_texts = misconception_texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = retrieval_model.encode_misconception(\n",
    "                encoded_input['input_ids'], \n",
    "                encoded_input['attention_mask']\n",
    "            )\n",
    "            batch_embeddings = F.normalize(model_output, p=2, dim=1)\n",
    "            misconception_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    misconception_embeddings = np.vstack(misconception_embeddings)\n",
    "    \n",
    "    # Step 2: Process each query\n",
    "    all_predictions = {}\n",
    "    \n",
    "    for query_idx, query_text in enumerate(tqdm(query_texts, desc=\"Processing queries\")):\n",
    "        # Tokenize query\n",
    "        encoded_query = tokenizer(\n",
    "            query_text, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=128, \n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate query embedding\n",
    "        with torch.no_grad():\n",
    "            query_embedding = retrieval_model.encode_query(\n",
    "                encoded_query['input_ids'], \n",
    "                encoded_query['attention_mask']\n",
    "            )\n",
    "            query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "            query_embedding = query_embedding.cpu().numpy()\n",
    "        \n",
    "        # Calculate similarity with all misconceptions\n",
    "        similarity_scores = np.dot(query_embedding, misconception_embeddings.T)[0]\n",
    "        \n",
    "        # Get top-k misconceptions for reranking\n",
    "        top_indices = np.argsort(similarity_scores)[-top_k*2:][::-1]  # Get more candidates for reranking\n",
    "        \n",
    "        # Prepare pairs for reranking\n",
    "        reranking_pairs = []\n",
    "        for misconception_idx in top_indices:\n",
    "            reranking_pairs.append((query_text, misconception_texts[misconception_idx]))\n",
    "        \n",
    "        # Rerank candidates\n",
    "        reranker_scores = []\n",
    "        reranker_batch_size = 16\n",
    "        \n",
    "        for i in range(0, len(reranking_pairs), reranker_batch_size):\n",
    "            batch_pairs = reranking_pairs[i:i+reranker_batch_size]\n",
    "            \n",
    "            # Tokenize pairs\n",
    "            encoded_pairs = tokenizer(\n",
    "                [p[0] for p in batch_pairs],\n",
    "                [p[1] for p in batch_pairs],\n",
    "                padding=True,\n",
    "                truncation='longest_first',\n",
    "                max_length=256,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            # Get scores\n",
    "            with torch.no_grad():\n",
    "                outputs = reranker_model(**encoded_pairs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                batch_scores = probs[:, 1].cpu().numpy()  # Probability of positive class\n",
    "                reranker_scores.extend(batch_scores)\n",
    "        \n",
    "        # Combine retrieval and reranking scores\n",
    "        alpha = 0.7  # Weight for reranking scores\n",
    "        combined_scores = []\n",
    "        \n",
    "        for i, misconception_idx in enumerate(top_indices):\n",
    "            retrieval_score = similarity_scores[misconception_idx]\n",
    "            reranking_score = reranker_scores[i]\n",
    "            combined_score = alpha * reranking_score + (1 - alpha) * retrieval_score\n",
    "            combined_scores.append((misconception_idx, combined_score))\n",
    "        \n",
    "        # Get final top-k predictions\n",
    "        final_predictions = sorted(combined_scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        # Store predictions\n",
    "        all_predictions[query_idx] = final_predictions\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a61b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a small subset of data for benchmarking\n",
    "n_samples = 10\n",
    "sample_queries = preprocessed_test['ProcessedQuery'].iloc[:n_samples].tolist()\n",
    "sample_misconceptions = preprocessed_misconceptions['ProcessedMisconceptionText'].iloc[:100].tolist()\n",
    "\n",
    "# Benchmark efficient pipeline\n",
    "start_time = time.time()\n",
    "efficient_predictions = efficient_inference_pipeline(\n",
    "    sample_queries,\n",
    "    sample_misconceptions,\n",
    "    tokenizer,\n",
    "    retrieval_model,\n",
    "    quantized_reranker,\n",
    "    top_k=25\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "time_per_query = total_time / n_samples\n",
    "\n",
    "print(f\"Total inference time for {n_samples} queries: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per query: {time_per_query:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971114c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(predictions, ground_truth, misconceptions_df, n_examples=5):\n",
    "    \"\"\"\n",
    "    Analyze errors made by the model.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary mapping QuestionId to list of (MisconceptionId, score) tuples\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        misconceptions_df: DataFrame with misconception information\n",
    "        n_examples: Number of examples to analyze\n",
    "    \"\"\"\n",
    "    # Group ground truth by QuestionId\n",
    "    gt_by_question = {}\n",
    "    for _, row in ground_truth.iterrows():\n",
    "        question_id = row['QuestionId']\n",
    "        misconception_id = row['MisconceptionId']\n",
    "        \n",
    "        if question_id not in gt_by_question:\n",
    "            gt_by_question[question_id] = []\n",
    "        \n",
    "        gt_by_question[question_id].append(misconception_id)\n",
    "    \n",
    "    # Find questions with errors\n",
    "    error_cases = []\n",
    "    \n",
    "    for question_id in gt_by_question:\n",
    "        if question_id not in predictions:\n",
    "            continue\n",
    "        \n",
    "        true_misconceptions = gt_by_question[question_id]\n",
    "        pred_misconceptions = [mid for mid, _ in predictions[question_id][:25]]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        n_correct = sum(1 for mid in pred_misconceptions if mid in true_misconceptions)\n",
    "        precision = n_correct / len(pred_misconceptions) if pred_misconceptions else 0\n",
    "        recall = n_correct / len(true_misconceptions) if true_misconceptions else 0\n",
    "        \n",
    "        # Check if there's an error\n",
    "        if precision < 1.0 or recall < 1.0:\n",
    "            error_cases.append({\n",
    "                'question_id': question_id,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'true_misconceptions': true_misconceptions,\n",
    "                'pred_misconceptions': pred_misconceptions\n",
    "            })\n",
    "    \n",
    "    # Sort by recall (prioritize cases where we missed true misconceptions)\n",
    "    error_cases.sort(key=lambda x: x['recall'])\n",
    "    \n",
    "    print(f\"Found {len(error_cases)} questions with errors (out of {len(gt_by_question)} total)\")\n",
    "    \n",
    "    # Analyze the worst cases\n",
    "    for i, case in enumerate(error_cases[:n_examples]):\n",
    "        print(f\"\\nError Case {i+1}:\")\n",
    "        question_id = case['question_id']\n",
    "        \n",
    "        # Get question text\n",
    "        question_text = ground_truth[ground_truth['QuestionId'] == question_id]['Query'].iloc[0]\n",
    "        print(f\"Question: {question_text}\")\n",
    "        \n",
    "        # Get true misconceptions\n",
    "        print(\"\\nTrue Misconceptions:\")\n",
    "        for j, mid in enumerate(case['true_misconceptions']):\n",
    "            misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "            found = mid in case['pred_misconceptions']\n",
    "            print(f\"  {j+1}. {mid}: {misconception_text} (Found: {found})\")\n",
    "        \n",
    "        # Get top 5 predicted misconceptions\n",
    "        print(\"\\nTop 5 Predicted Misconceptions:\")\n",
    "        for j, mid in enumerate(case['pred_misconceptions'][:5]):\n",
    "            misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "            correct = mid in case['true_misconceptions']\n",
    "            print(f\"  {j+1}. {mid}: {misconception_text} (Correct: {correct})\")\n",
    "        \n",
    "        print(f\"\\nMetrics - Precision: {case['precision']:.2f}, Recall: {case['recall']:.2f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    analyze_error_patterns(error_cases, ground_truth, misconceptions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc316a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_patterns(error_cases, ground_truth, misconceptions_df):\n",
    "    \"\"\"\n",
    "    Analyze common error patterns.\n",
    "    \n",
    "    Args:\n",
    "        error_cases: List of error case dictionaries\n",
    "        ground_truth: DataFrame with QuestionId and MisconceptionId columns\n",
    "        misconceptions_df: DataFrame with misconception information\n",
    "    \"\"\"\n",
    "    # Collect false positives and false negatives\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    \n",
    "    for case in error_cases:\n",
    "        true_misconceptions = case['true_misconceptions']\n",
    "        pred_misconceptions = case['pred_misconceptions']\n",
    "        \n",
    "        # Get false positives and false negatives\n",
    "        fps = [mid for mid in pred_misconceptions if mid not in true_misconceptions]\n",
    "        fns = [mid for mid in true_misconceptions if mid not in pred_misconceptions]\n",
    "        \n",
    "        false_positives.extend(fps)\n",
    "        false_negatives.extend(fns)\n",
    "    \n",
    "    # Count occurrences of misconceptions\n",
    "    fp_counts = pd.Series(false_positives).value_counts()\n",
    "    fn_counts = pd.Series(false_negatives).value_counts()\n",
    "    \n",
    "    # Analyze most common false positives\n",
    "    print(\"\\nMost Common False Positives:\")\n",
    "    for mid, count in fp_counts.head(10).items():\n",
    "        misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "        print(f\"  {mid}: {misconception_text} (Count: {count})\")\n",
    "    \n",
    "    # Analyze most common false negatives\n",
    "    print(\"\\nMost Common False Negatives:\")\n",
    "    for mid, count in fn_counts.head(10).items():\n",
    "        misconception_text = misconceptions_df[misconceptions_df['MisconceptionId'] == mid]['MisconceptionText'].iloc[0]\n",
    "        print(f\"  {mid}: {misconception_text} (Count: {count})\")\n",
    "    \n",
    "    # Analyze if there's a correlation between misconception frequency and errors\n",
    "    print(\"\\nMisconception Frequency Analysis:\")\n",
    "    \n",
    "    # Get misconception frequencies in the training data\n",
    "    misconception_freq = ground_truth['MisconceptionId'].value_counts()\n",
    "    \n",
    "    # Calculate average frequency of true positives, false positives, and false negatives\n",
    "    all_true_positives = []\n",
    "    for case in error_cases:\n",
    "        true_misconceptions = case['true_misconceptions']\n",
    "        pred_misconceptions = case['pred_misconceptions']\n",
    "        tps = [mid for mid in pred_misconceptions if mid in true_misconceptions]\n",
    "        all_true_positives.extend(tps)\n",
    "    \n",
    "    tp_freq = [misconception_freq.get(mid, 0) for mid in all_true_positives]\n",
    "    fp_freq = [misconception_freq.get(mid, 0) for mid in false_positives]\n",
    "    fn_freq = [misconception_freq.get(mid, 0) for mid in false_negatives]\n",
    "    \n",
    "    print(f\"  Average frequency of true positives: {np.mean(tp_freq):.2f}\")\n",
    "    print(f\"  Average frequency of false positives: {np.mean(fp_freq):.2f}\")\n",
    "    print(f\"  Average frequency of false negatives: {np.mean(fn_freq):.2f}\")\n",
    "    \n",
    "    # Check if errors are more common for unseen misconceptions\n",
    "    seen_misconceptions = set(ground_truth['MisconceptionId'])\n",
    "    unseen_fps = [mid for mid in false_positives if mid not in seen_misconceptions]\n",
    "    unseen_fns = [mid for mid in false_negatives if mid not in seen_misconceptions]\n",
    "    \n",
    "    print(f\"\\nUnseen Misconception Analysis:\")\n",
    "    print(f\"  Proportion of false positives that are unseen: {len(unseen_fps) / len(false_positives):.2f}\")\n",
    "    print(f\"  Proportion of false negatives that are unseen: {len(unseen_fns) / len(false_negatives):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc707a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a validation split for error analysis\n",
    "# This would typically come from a cross-validation fold\n",
    "from section_3_functions import calculate_map_at_k\n",
    "\n",
    "# Get a validation split\n",
    "unseen_train_idx, unseen_valid_idx = cv_splits['unseen_split']\n",
    "train_subset = preprocessed_train.iloc[unseen_train_idx]\n",
    "valid_subset = preprocessed_train.iloc[unseen_valid_idx]\n",
    "\n",
    "# Get predictions for validation set\n",
    "with open('validation_predictions.pkl', 'rb') as f:\n",
    "    validation_predictions = pickle.load(f)\n",
    "\n",
    "# Calculate MAP@25\n",
    "map_score = calculate_map_at_k(validation_predictions, valid_subset, k=25)\n",
    "print(f\"Validation MAP@25: {map_score:.4f}\")\n",
    "\n",
    "# Analyze errors\n",
    "analyze_errors(validation_predictions, valid_subset, preprocessed_misconceptions, n_examples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d9238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize overall performance across different model configurations\n",
    "model_performances = {\n",
    "    'TF-IDF Baseline': 0.35,\n",
    "    'Simple Retrieval (MPNet)': 0.42,\n",
    "    'Retrieval + Reranking': 0.47,\n",
    "    'Ensemble': 0.51,\n",
    "    'Optimized Ensemble': 0.53\n",
    "}\n",
    "\n",
    "# Create a bar chart of model performances\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = list(model_performances.keys())\n",
    "scores = list(model_performances.values())\n",
    "\n",
    "plt.bar(models, scores, color='skyblue')\n",
    "plt.title('Model Performance Comparison (MAP@25)')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('MAP@25')\n",
    "plt.ylim(0, 0.6)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    plt.text(i, score + 0.01, f'{score:.2f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
